{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpilo\\Hello\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22b9e9db8d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_day</th>\n",
       "      <th>date_month</th>\n",
       "      <th>date_year</th>\n",
       "      <th>Week</th>\n",
       "      <th>Spread</th>\n",
       "      <th>OverUnder</th>\n",
       "      <th>days_rest</th>\n",
       "      <th>team_season_wins</th>\n",
       "      <th>team_season_losses</th>\n",
       "      <th>prev_week_total</th>\n",
       "      <th>...</th>\n",
       "      <th>Sun</th>\n",
       "      <th>Thu</th>\n",
       "      <th>Tue</th>\n",
       "      <th>Wed</th>\n",
       "      <th>spread_Lost</th>\n",
       "      <th>spread_Push</th>\n",
       "      <th>spread_Won</th>\n",
       "      <th>OU_Over</th>\n",
       "      <th>OU_Push</th>\n",
       "      <th>OU_Under</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>2009</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>2009</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6161</th>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>2.5</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6162</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>6.5</td>\n",
       "      <td>43.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6163</th>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>43.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6164</th>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>14</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165</th>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5720 rows × 611 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_day  date_month  date_year  Week  Spread  OverUnder  days_rest  \\\n",
       "1           20           9       2009     2     3.0       44.0        7.0   \n",
       "2           27           9       2009     3    -3.0       49.0        7.0   \n",
       "3           11          10       2009     4    -6.0       51.0       14.0   \n",
       "4           18          10       2009     5     3.0       46.0        7.0   \n",
       "5           25          10       2009     6     8.0       47.0        7.0   \n",
       "...        ...         ...        ...   ...     ...        ...        ...   \n",
       "6161        26          11       2020    11     2.5       46.0        4.0   \n",
       "6162         7          12       2020    12     6.5       43.5       11.0   \n",
       "6163        13          12       2020    13     3.0       43.5        6.0   \n",
       "6164        20          12       2020    14     6.0       44.5        7.0   \n",
       "6165        27          12       2020    15     1.0       41.5        7.0   \n",
       "\n",
       "      team_season_wins  team_season_losses  prev_week_total  ...  Sun  Thu  \\\n",
       "1                  0.0                 1.0             36.0  ...    1    0   \n",
       "2                  1.0                 1.0             48.0  ...    1    0   \n",
       "3                  1.0                 2.0             41.0  ...    1    0   \n",
       "4                  2.0                 2.0             49.0  ...    1    0   \n",
       "5                  3.0                 2.0             30.0  ...    1    0   \n",
       "...                ...                 ...              ...  ...  ...  ...   \n",
       "6161               3.0                 7.0             29.0  ...    0    1   \n",
       "6162               4.0                 7.0             57.0  ...    0    0   \n",
       "6163               5.0                 7.0             40.0  ...    1    0   \n",
       "6164               6.0                 7.0             38.0  ...    1    0   \n",
       "6165               6.0                 8.0             35.0  ...    1    0   \n",
       "\n",
       "      Tue  Wed  spread_Lost  spread_Push  spread_Won  OU_Over  OU_Push  \\\n",
       "1       0    0            0            0           1        1        0   \n",
       "2       0    0            1            0           0        0        0   \n",
       "3       0    0            0            0           1        0        0   \n",
       "4       0    0            0            0           1        0        0   \n",
       "5       0    0            0            0           1        0        0   \n",
       "...   ...  ...          ...          ...         ...      ...      ...   \n",
       "6161    0    0            0            0           1        1        0   \n",
       "6162    0    0            0            0           1        0        0   \n",
       "6163    0    0            0            0           1        0        0   \n",
       "6164    0    0            0            0           1        0        0   \n",
       "6165    0    0            1            0           0        0        0   \n",
       "\n",
       "      OU_Under  \n",
       "1            0  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "5            1  \n",
       "...        ...  \n",
       "6161         0  \n",
       "6162         1  \n",
       "6163         1  \n",
       "6164         1  \n",
       "6165         1  \n",
       "\n",
       "[5720 rows x 611 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\cpilo\\Downloads\\df_all_teams_070823.csv\")\n",
    "\n",
    "df_team_data = pd.get_dummies(df.team_abbr)\n",
    "df_opponent_data = pd.get_dummies(df.opponent_abbr)\n",
    "col_names = []\n",
    "for i in df_opponent_data.columns:\n",
    "    col_names.append('opp_'+i)\n",
    "df_opponent_data.columns = col_names\n",
    "game_city = pd.get_dummies(df.game_city)\n",
    "game_time = pd.get_dummies(df.Time)\n",
    "game_day = pd.get_dummies(df.Day)\n",
    "vs_line = pd.get_dummies(df['vs. Line'])\n",
    "qb_1 = pd.get_dummies(df['QB1'])\n",
    "opp_qb_1 = pd.get_dummies(df['opp_QB1'])\n",
    "skill_1 = pd.get_dummies(df['Skill1'])\n",
    "opp_skill_1 = pd.get_dummies(df['opp_skill1'])\n",
    "skill_2 = pd.get_dummies(df['Skill2'])\n",
    "opp_skill_2 = pd.get_dummies(df['opp_skill2'])\n",
    "skill_3 = pd.get_dummies(df['Skill3'])\n",
    "opp_skill_3 = pd.get_dummies(df['opp_skill3'])\n",
    "skill_4 = pd.get_dummies(df['Skill4'])\n",
    "opp_skill_4 = pd.get_dummies(df['opp_skill4'])\n",
    "skill_5 = pd.get_dummies(df['Skill5'])\n",
    "opp_skill_5 = pd.get_dummies(df['opp_skill5'])\n",
    "oline_1 = pd.get_dummies(df['Oline1'])\n",
    "opp_oline_1 = pd.get_dummies(df['opp_oline1'])\n",
    "oline_2 = pd.get_dummies(df['Oline2'])\n",
    "opp_oline_2 = pd.get_dummies(df['opp_oline2'])\n",
    "oline_3 = pd.get_dummies(df['Oline3'])\n",
    "opp_oline_3 = pd.get_dummies(df['opp_oline3'])\n",
    "oline_4 = pd.get_dummies(df['Oline4'])\n",
    "opp_oline_4 = pd.get_dummies(df['opp_oline4'])\n",
    "oline_5 = pd.get_dummies(df['Oline5'])\n",
    "opp_oline_5 = pd.get_dummies(df['opp_oline5'])\n",
    "dline_1 = pd.get_dummies(df['Dline1'])\n",
    "opp_dline_1 = pd.get_dummies(df['opp_dline1'])\n",
    "dline_2 = pd.get_dummies(df['Dline2'])\n",
    "opp_dline_2 = pd.get_dummies(df['opp_dline2'])\n",
    "dline_3 = pd.get_dummies(df['Dline3'])\n",
    "opp_dline_3 = pd.get_dummies(df['opp_dline3'])\n",
    "lb_1 = pd.get_dummies(df['LB1'])\n",
    "opp_lb_1 = pd.get_dummies(df['opp_lb1'])\n",
    "lb_2 = pd.get_dummies(df['LB2'])\n",
    "opp_lb_2 = pd.get_dummies(df['opp_lb2'])\n",
    "lb_3 = pd.get_dummies(df['LB3'])\n",
    "opp_lb_3 = pd.get_dummies(df['opp_lb3'])\n",
    "lb_4 = pd.get_dummies(df['LB4'])\n",
    "opp_lb_4 = pd.get_dummies(df['opp_lb4'])\n",
    "cb_1 = pd.get_dummies(df['CB1'])\n",
    "opp_cb_1 = pd.get_dummies(df['opp_cb1'])\n",
    "cb_2 = pd.get_dummies(df['CB2'])\n",
    "opp_cb_2 = pd.get_dummies(df['opp_cb2'])\n",
    "ss = pd.get_dummies(df['SS'])\n",
    "opp_ss = pd.get_dummies(df['opp_ss'])\n",
    "fs = pd.get_dummies(df['FS'])\n",
    "opp_fs = pd.get_dummies(df['opp_fs'])\n",
    "\n",
    "col_names = []\n",
    "for i in vs_line.columns:\n",
    "    col_names.append('spread_'+i)\n",
    "vs_line.columns = col_names\n",
    "OU_result = pd.get_dummies(df['OU Result'])\n",
    "col_names = []\n",
    "for i in OU_result.columns:\n",
    "    col_names.append('OU_'+i)\n",
    "OU_result.columns = col_names\n",
    "model_vars = ['date_day', 'date_month', 'date_year', 'Week', 'Spread', 'OverUnder','days_rest', 'team_season_wins', \n",
    "              'team_season_losses', 'prev_week_total', 'prev_week_ptsdiff',\n",
    "              'prev_week_ydsoff', 'prev_week_ydsdef', 'prev_week_ptsoff', \n",
    "              'prev_week_ptsdef', 'prev_week_TOoff', 'prev_week_TOdef', \n",
    "              'prev_week_seasptsdiff', 'prev_week_seasptsoff', \n",
    "              'prev_week_seasptsdef', 'prev_week_seasydsoff', 'prev_week_seasydsdef', \n",
    "              'prev_week_seasTOoff', 'prev_week_seasTOdef', 'prev_week_avgpts', 'prev_week_TOdiff',\n",
    "              'prev_week_avgTOdiff', 'prev_week_avgptsdiff', 'prev_week_avgptsoff', \n",
    "              'prev_week_avgptsdef', 'prev_week_avgydsoff', 'prev_week_avgydsdef', \n",
    "              'prev_week_avgTOoff', 'prev_week_avgTOdef', 'Opp_days_rest', 'Opp_team_season_wins',\n",
    "              'Opp_team_season_losses', 'opp_prev_week_total', 'opp_prev_week_pts_diff', \n",
    "              'opp_prev_week_ydsoff', 'opp_prev_week_ydsdef', 'opp_prev_week_ptsoff', 'opp_prev_week_ptsdef', \n",
    "              'opp_prev_week_TOoff', 'opp_prev_week_TOdef', 'opp_prev_week_seasptsdiff', \n",
    "              'opp_prev_week_seasptsoff', 'opp_prev_week_seasptsdef', 'opp_prev_week_seasydsoff',\n",
    "              'opp_prev_week_seasydsdef', 'opp_prev_week_seasTOoff', 'opp_prev_week_seasTOdef',\n",
    "              'opp_prev_week_avgpts', 'opp_prev_week_TOdiff', 'opp_prev_week_avgTOdiff',\n",
    "              'opp_prev_week_avgptsdiff', 'opp_prev_week_avgptsoff', 'opp_prev_week_avgptsdef',\n",
    "              'opp_prev_week_avgydsoff', 'opp_prev_week_avgydsdef', 'opp_prev_week_avgTOoff',\n",
    "              'opp_prev_week_avgTOdef', 'prev_week_avgspreaddif', 'prev_week_avgoverdif', 'prev_week_avgspread',\n",
    "              'prev_week_avgover', 'prev_week_seasspreadwin', 'prev_week_seasspreadloss', 'prev_week_seasspreadtie',\n",
    "              'prev_week_seasoverwin', 'prev_week_seasoverloss', 'prev_week_seasovertie', 'prev_week_seastotspread',\n",
    "              'prev_week_seastotover', 'prev_week_spreadwin', 'prev_week_spreadloss', 'prev_week_spreadtie',\n",
    "              'prev_week_overwin', 'prev_week_overloss', 'prev_week_overtie', 'prev_week_spreaddiff', \n",
    "              'prev_week_overdiff', 'opp_prev_week_avgspreaddif', 'opp_prev_week_avgoverdif',\n",
    "              'prev_week_streakw', 'prev_week_streakl', 'prev_week_streakspreadw', 'prev_week_streakspreadl',\n",
    "              'prev_week_streakoverw', 'prev_week_streakoverl', 'prev_week_avgspread', 'prev_week_avgover', \n",
    "              'prev_week_seasspreadwin', 'prev_week_seasspreadloss', 'prev_week_seasspreadtie', 'prev_week_seasoverwin',\n",
    "              'prev_week_seasoverloss', 'prev_week_seasovertie', 'prev_week_seastotspread', 'prev_week_seastotover',\n",
    "              'prev_week_spreadwin', 'prev_week_spreadloss', 'prev_week_spreadtie', 'prev_week_overwin',\n",
    "              'prev_week_overloss', 'prev_week_overtie', 'prev_week_spreaddiff', 'prev_week_overdiff',              \n",
    "              'opp_prev_week_avgspread', 'opp_prev_week_avgover', 'opp_prev_week_seasspreadwin', \n",
    "              'opp_prev_week_seasspreadloss', 'opp_prev_week_seasspreadtie', 'opp_prev_week_seasoverwin',\n",
    "              'opp_prev_week_seasoverloss', 'opp_prev_week_seasovertie', 'opp_prev_week_seastotspread', \n",
    "              'opp_prev_week_seastotover', 'opp_prev_week_spreadwin', 'opp_prev_week_spreadloss', \n",
    "              'opp_prev_week_spreadtie', 'opp_prev_week_overwin', 'opp_prev_week_overloss', \n",
    "              'opp_prev_week_overtie', 'opp_prev_week_spreaddiff', 'opp_prev_week_overdiff',\n",
    "              'opp_prev_week_avgspread', 'opp_prev_week_avgover', 'opp_prev_week_seasspreadwin',\n",
    "              'opp_prev_week_seasspreadloss', 'opp_prev_week_seasspreadtie', 'opp_prev_week_seasoverwin',\n",
    "              'opp_prev_week_seasoverloss', 'opp_prev_week_seasovertie', 'opp_prev_week_seastotspread', \n",
    "              'opp_prev_week_seastotover', 'opp_prev_week_spreadwin', 'opp_prev_week_spreadloss',\n",
    "              'opp_prev_week_spreadtie', 'opp_prev_week_overwin', 'opp_prev_week_overloss',\n",
    "              'opp_prev_week_overtie', 'opp_prev_week_spreaddiff', 'opp_prev_week_overdiff',\n",
    "              'opp_prev_week_streakw', 'opp_prev_week_streakl', 'opp_prev_week_streakspreadw',\n",
    "              'opp_prev_week_streakspreadl', 'opp_prev_week_streakoverw', 'opp_prev_week_streakoverl']\n",
    "              \n",
    "\n",
    "df_rfc_data = df[model_vars]\n",
    "df_final_data = pd.concat([df_rfc_data, df_team_data, df_opponent_data, game_city, qb_1, opp_qb_1,\n",
    "                           skill_1, opp_skill_1, skill_2, opp_skill_2, skill_3, opp_skill_3,\n",
    "                           skill_4, opp_skill_4, skill_5, opp_skill_5, oline_1, opp_oline_1,\n",
    "                           oline_2, opp_oline_2, oline_3, opp_oline_3, oline_4, opp_oline_4,\n",
    "                           oline_5, opp_oline_5, dline_1, opp_dline_1, dline_2, opp_dline_2,\n",
    "                           dline_3, opp_dline_3, lb_1, opp_lb_1, lb_2, opp_lb_2, lb_3, opp_lb_3, \n",
    "                           lb_4, opp_lb_4, cb_1, opp_cb_1, cb_2, opp_cb_2, ss, opp_ss, fs, opp_fs,\n",
    "                           game_time, game_day, vs_line, OU_result], axis = 'columns')\n",
    "\n",
    "df_final_data = df_final_data[df_final_data['Week'] != 1]\n",
    "df_final_data = df_final_data.dropna(axis = 'index')\n",
    "df_train = df_final_data[df_final_data['date_year'] < 2021]\n",
    "df_test = df_final_data[df_final_data['date_year'] >= 2021]\n",
    "df_train = df_train.replace({True:1, False:0})\n",
    "df_test = df_test.replace({True:1, False:0})\n",
    "print(len(df_train.columns))\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col_names = df_train.drop(['spread_Won', 'spread_Lost', 'spread_Push', 'OU_Over', 'OU_Push', 'OU_Under'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_day</th>\n",
       "      <th>date_month</th>\n",
       "      <th>date_year</th>\n",
       "      <th>Week</th>\n",
       "      <th>Spread</th>\n",
       "      <th>OverUnder</th>\n",
       "      <th>days_rest</th>\n",
       "      <th>team_season_wins</th>\n",
       "      <th>team_season_losses</th>\n",
       "      <th>prev_week_total</th>\n",
       "      <th>...</th>\n",
       "      <th>9:35AM</th>\n",
       "      <th>9:36AM</th>\n",
       "      <th>9:37AM</th>\n",
       "      <th>Fri</th>\n",
       "      <th>Mon</th>\n",
       "      <th>Sat</th>\n",
       "      <th>Sun</th>\n",
       "      <th>Thu</th>\n",
       "      <th>Tue</th>\n",
       "      <th>Wed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>2009</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>2009</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6161</th>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>2.5</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6162</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>6.5</td>\n",
       "      <td>43.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6163</th>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>43.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6164</th>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>14</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165</th>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5720 rows × 605 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_day  date_month  date_year  Week  Spread  OverUnder  days_rest  \\\n",
       "1           20           9       2009     2     3.0       44.0        7.0   \n",
       "2           27           9       2009     3    -3.0       49.0        7.0   \n",
       "3           11          10       2009     4    -6.0       51.0       14.0   \n",
       "4           18          10       2009     5     3.0       46.0        7.0   \n",
       "5           25          10       2009     6     8.0       47.0        7.0   \n",
       "...        ...         ...        ...   ...     ...        ...        ...   \n",
       "6161        26          11       2020    11     2.5       46.0        4.0   \n",
       "6162         7          12       2020    12     6.5       43.5       11.0   \n",
       "6163        13          12       2020    13     3.0       43.5        6.0   \n",
       "6164        20          12       2020    14     6.0       44.5        7.0   \n",
       "6165        27          12       2020    15     1.0       41.5        7.0   \n",
       "\n",
       "      team_season_wins  team_season_losses  prev_week_total  ...  9:35AM  \\\n",
       "1                  0.0                 1.0             36.0  ...       0   \n",
       "2                  1.0                 1.0             48.0  ...       0   \n",
       "3                  1.0                 2.0             41.0  ...       0   \n",
       "4                  2.0                 2.0             49.0  ...       0   \n",
       "5                  3.0                 2.0             30.0  ...       0   \n",
       "...                ...                 ...              ...  ...     ...   \n",
       "6161               3.0                 7.0             29.0  ...       0   \n",
       "6162               4.0                 7.0             57.0  ...       0   \n",
       "6163               5.0                 7.0             40.0  ...       0   \n",
       "6164               6.0                 7.0             38.0  ...       0   \n",
       "6165               6.0                 8.0             35.0  ...       0   \n",
       "\n",
       "      9:36AM  9:37AM  Fri  Mon  Sat  Sun  Thu  Tue  Wed  \n",
       "1          0       0    0    0    0    1    0    0    0  \n",
       "2          0       0    0    0    0    1    0    0    0  \n",
       "3          0       0    0    0    0    1    0    0    0  \n",
       "4          0       0    0    0    0    1    0    0    0  \n",
       "5          0       0    0    0    0    1    0    0    0  \n",
       "...      ...     ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "6161       0       0    0    0    0    0    1    0    0  \n",
       "6162       0       0    0    1    0    0    0    0    0  \n",
       "6163       0       0    0    0    0    1    0    0    0  \n",
       "6164       0       0    0    0    0    1    0    0    0  \n",
       "6165       0       0    0    0    0    1    0    0    0  \n",
       "\n",
       "[5720 rows x 605 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor(df_train['OU_Over'].values).float()\n",
    "X = torch.tensor(df_train.drop(['spread_Won', 'spread_Lost', 'spread_Push', 'OU_Over', 'OU_Push', 'OU_Under'], \n",
    "                               axis = 'columns').values).float()\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size = 0.3)\n",
    "y_test = torch.tensor(df_test['OU_Over'].values).float()\n",
    "x_test = torch.tensor(df_test.drop(['spread_Won', 'spread_Lost', 'spread_Push', 'OU_Over', 'OU_Push', 'OU_Under'], \n",
    "                                   axis = 'columns').values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0000e+00, 1.0000e+00, 2.0210e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [3.0000e+00, 1.0000e+00, 2.0210e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [3.0000e+00, 1.0000e+00, 2.0210e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [2.6000e+01, 1.2000e+01, 2.0210e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [2.0000e+00, 1.0000e+00, 2.0220e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [9.0000e+00, 1.0000e+00, 2.0220e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_x = pd.DataFrame(x_train.numpy())\n",
    "training_x.to_csv(r\"C:\\Users\\cpilo\\Documents\\NFL_Models\\x_train.csv\", index = False)\n",
    "training_y = pd.Series(y_train.numpy())\n",
    "training_y.to_csv(r\"C:\\Users\\cpilo\\Documents\\NFL_Models\\y_train.csv\", index = False)\n",
    "val_x = pd.DataFrame(x_val.numpy())\n",
    "val_x.to_csv(r\"C:\\Users\\cpilo\\Documents\\NFL_Models\\x_val.csv\", index = False)\n",
    "val_y = pd.Series(y_val.numpy())\n",
    "val_y.to_csv(r\"C:\\Users\\cpilo\\Documents\\NFL_Models\\y_val.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model for relu activation\n",
    "class neural_net(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, D_out):#H4, H5, H6, D_out):\n",
    "        super(neural_net, self).__init__()\n",
    "        #Define our leaky relu activation function\n",
    "        self.relu = nn.LeakyReLU(0.01)\n",
    "        #define our sigmoid function for our output layer\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        #we will apply batch normalization to the output of each hidden layer\n",
    "        self.batch1 = nn.BatchNorm1d(H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.batch2 = nn.BatchNorm1d(H2)\n",
    "        self.linear3 = nn.Linear(H2, H3)\n",
    "        self.batch3 = nn.BatchNorm1d(H3)\n",
    "        self.linear4 = nn.Linear(H3, D_out)\n",
    "    def forward(self, x):\n",
    "        #call our network\n",
    "        #we apply batch normalization before our activation function\n",
    "        x = self.relu(self.batch1(self.linear1(x)))\n",
    "        x = self.relu(self.batch2(self.linear2(x)))\n",
    "        x = self.relu(self.batch3(self.linear3(x)))\n",
    "        x = self.sig(self.linear4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "# define training function\n",
    "def train(model, criterion, x_train, x_val, y_train, y_val, optimizer, epochs = 100):\n",
    "    i = 0\n",
    "    useful_stuff = {'training_loss': [], 'validation_accuracy': []}\n",
    "    for epoch in range(epochs):\n",
    "        total = 0\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(x_train)\n",
    "        loss = criterion(yhat, y_train.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        useful_stuff['training_loss'].append(loss.item())\n",
    "        #validation\n",
    "        z = model(x_val)\n",
    "        MSE = criterion(z, y_val.unsqueeze(1))\n",
    "        useful_stuff['validation_accuracy'].append(MSE)\n",
    "        if (epoch%200==0):\n",
    "            print('epoch {}, loss {}'.format(epoch, loss.item()/1))\n",
    "        \n",
    "    return useful_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create criterion function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.2566361427307129\n",
      "epoch 200, loss 0.2565983831882477\n",
      "epoch 400, loss 0.2565596103668213\n",
      "epoch 600, loss 0.2565206289291382\n",
      "epoch 800, loss 0.2564818561077118\n",
      "epoch 1000, loss 0.25644299387931824\n",
      "epoch 1200, loss 0.2564041316509247\n",
      "epoch 1400, loss 0.25636571645736694\n",
      "epoch 1600, loss 0.2563268840312958\n",
      "epoch 1800, loss 0.2562876343727112\n",
      "epoch 2000, loss 0.2562485337257385\n",
      "epoch 2200, loss 0.2562089264392853\n",
      "epoch 2400, loss 0.2561686933040619\n"
     ]
    }
   ],
   "source": [
    "# train model with relu\n",
    "model = neural_net(605, 235, 100, 40, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.000001)\n",
    "training_results = train(model, criterion, x_train, x_val, y_train, y_val, optimizer, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2561485767364502"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_results['training_loss'][len(training_results['training_loss'])-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 8000\n",
    "loss = training_results['training_loss'][len(training_results['training_loss'])-1]\n",
    "PATH = r'C:\\Users\\cpilo\\Documents\\NFL_Models\\nfl_batch_model1'\n",
    "#save our model\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear1.weight',\n",
       "              tensor([[ 0.0093, -0.0097,  0.0111,  ..., -0.0032,  0.0351,  0.0035],\n",
       "                      [-0.0149, -0.0063, -0.0359,  ...,  0.0256,  0.0210,  0.0283],\n",
       "                      [-0.0061,  0.0085,  0.0003,  ..., -0.0248, -0.0059, -0.0227],\n",
       "                      ...,\n",
       "                      [ 0.0084,  0.0354,  0.0326,  ...,  0.0044, -0.0214,  0.0067],\n",
       "                      [-0.0034, -0.0237,  0.0043,  ...,  0.0362,  0.0141,  0.0185],\n",
       "                      [-0.0162,  0.0178, -0.0222,  ..., -0.0211, -0.0069,  0.0194]])),\n",
       "             ('linear1.bias',\n",
       "              tensor([-0.0261,  0.0360, -0.0307, -0.0211, -0.0145, -0.0137,  0.0054, -0.0342,\n",
       "                      -0.0303,  0.0081,  0.0090,  0.0391,  0.0246, -0.0144, -0.0052, -0.0096,\n",
       "                      -0.0098,  0.0393, -0.0057,  0.0137, -0.0088,  0.0092,  0.0126,  0.0022,\n",
       "                      -0.0275, -0.0284, -0.0109,  0.0014,  0.0117,  0.0254, -0.0023,  0.0243,\n",
       "                      -0.0154,  0.0004, -0.0200,  0.0145, -0.0197, -0.0241, -0.0366, -0.0284,\n",
       "                       0.0383, -0.0193, -0.0148, -0.0243,  0.0391,  0.0231, -0.0123,  0.0109,\n",
       "                       0.0188, -0.0355,  0.0325,  0.0357, -0.0060,  0.0318, -0.0036, -0.0192,\n",
       "                       0.0250, -0.0187,  0.0334, -0.0072, -0.0202,  0.0403,  0.0127, -0.0319,\n",
       "                      -0.0141,  0.0134, -0.0314, -0.0386, -0.0066,  0.0381,  0.0301, -0.0182,\n",
       "                       0.0229, -0.0260, -0.0064,  0.0062, -0.0223,  0.0168, -0.0349, -0.0066,\n",
       "                      -0.0093,  0.0102, -0.0101,  0.0052, -0.0203, -0.0286, -0.0188, -0.0098,\n",
       "                      -0.0328, -0.0103,  0.0324,  0.0301, -0.0306,  0.0074, -0.0070,  0.0143,\n",
       "                      -0.0114, -0.0385, -0.0390, -0.0309,  0.0245, -0.0082, -0.0142, -0.0291,\n",
       "                       0.0290,  0.0170,  0.0211,  0.0060,  0.0218,  0.0316, -0.0149,  0.0278,\n",
       "                       0.0146,  0.0052, -0.0049,  0.0157,  0.0330, -0.0075, -0.0075,  0.0378,\n",
       "                       0.0373, -0.0189,  0.0056, -0.0136, -0.0402, -0.0151,  0.0019, -0.0327,\n",
       "                      -0.0233, -0.0293, -0.0098, -0.0104, -0.0171,  0.0319, -0.0350, -0.0077,\n",
       "                       0.0333,  0.0063, -0.0251,  0.0277, -0.0318, -0.0153,  0.0321,  0.0251,\n",
       "                       0.0033,  0.0125, -0.0066, -0.0339,  0.0165, -0.0263,  0.0226,  0.0147,\n",
       "                      -0.0124,  0.0057,  0.0106,  0.0179, -0.0055,  0.0126, -0.0379,  0.0049,\n",
       "                       0.0130, -0.0364, -0.0165,  0.0011, -0.0003, -0.0239, -0.0351,  0.0281,\n",
       "                      -0.0343, -0.0017,  0.0334, -0.0088, -0.0085, -0.0278,  0.0271,  0.0018,\n",
       "                      -0.0307,  0.0053,  0.0359,  0.0246, -0.0127, -0.0204,  0.0246,  0.0105,\n",
       "                       0.0238, -0.0181, -0.0220,  0.0236,  0.0024,  0.0111,  0.0225,  0.0400,\n",
       "                       0.0164,  0.0231,  0.0312, -0.0292,  0.0393, -0.0398,  0.0063, -0.0230,\n",
       "                       0.0071, -0.0163, -0.0094, -0.0039,  0.0269,  0.0249,  0.0062,  0.0132,\n",
       "                       0.0198,  0.0260, -0.0302,  0.0299,  0.0068,  0.0027,  0.0210,  0.0121,\n",
       "                      -0.0200,  0.0050, -0.0021, -0.0272,  0.0244, -0.0197,  0.0334,  0.0232,\n",
       "                      -0.0265,  0.0344, -0.0190, -0.0082, -0.0287,  0.0107, -0.0206,  0.0090,\n",
       "                       0.0208, -0.0070, -0.0014])),\n",
       "             ('batch1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1.])),\n",
       "             ('batch1.bias',\n",
       "              tensor([-8.7568e-08,  8.1622e-07,  5.1634e-07, -1.7802e-07,  4.2740e-08,\n",
       "                      -5.9221e-07,  1.0463e-07,  7.3632e-07,  3.6483e-07,  2.2382e-07,\n",
       "                      -1.0244e-07,  2.7068e-07, -3.5032e-07, -3.9866e-07, -5.0464e-07,\n",
       "                       6.9597e-08, -9.2440e-07,  2.1746e-07, -3.7782e-07,  6.2752e-07,\n",
       "                       2.1400e-08, -2.8257e-07,  3.5599e-07, -5.1780e-07,  2.3759e-07,\n",
       "                       1.3862e-09, -9.7663e-07, -3.8959e-07,  3.6905e-08,  5.7845e-07,\n",
       "                      -5.4163e-07,  1.7199e-07,  2.6558e-07,  5.4187e-08, -4.8316e-08,\n",
       "                       4.0875e-07, -4.2005e-07, -4.6824e-07,  1.6807e-07,  4.8622e-07,\n",
       "                       2.4506e-07,  3.2887e-07,  1.8370e-07,  9.3898e-07,  2.2714e-07,\n",
       "                      -6.1343e-07, -7.7130e-08,  1.3321e-07, -1.8196e-07, -6.6288e-08,\n",
       "                      -4.8790e-07, -4.2376e-07,  3.9530e-07,  2.7368e-07, -1.4716e-07,\n",
       "                      -1.8556e-07,  2.1011e-07,  3.8509e-07, -4.4793e-08, -1.1879e-07,\n",
       "                       5.3437e-07,  6.0029e-07, -6.7958e-07,  5.8076e-09, -2.4536e-07,\n",
       "                       8.1789e-08,  3.6566e-08,  1.0316e-06, -2.7524e-07, -5.3320e-07,\n",
       "                       5.8145e-07, -1.7958e-08,  8.6857e-07, -3.0206e-07,  1.1160e-06,\n",
       "                      -3.6327e-07, -7.5124e-08,  1.6929e-07, -1.3880e-06, -2.6638e-07,\n",
       "                      -2.8781e-07,  1.2476e-07, -2.5858e-07, -8.3680e-07,  2.1668e-07,\n",
       "                      -3.0268e-07,  2.9039e-07, -1.7007e-07,  2.7284e-07, -5.5291e-07,\n",
       "                      -4.2741e-07, -4.9795e-07,  3.6680e-08, -9.2359e-08,  2.9317e-07,\n",
       "                      -2.4040e-07, -1.4360e-07,  7.4339e-07,  5.2832e-08,  2.5201e-07,\n",
       "                       4.5343e-07, -1.6013e-08,  8.5673e-08, -2.7679e-07, -2.6013e-07,\n",
       "                      -7.6917e-07,  7.6982e-08, -6.5104e-07,  5.4510e-07,  4.4089e-07,\n",
       "                      -3.9773e-07, -1.6111e-07,  3.7309e-07,  1.5943e-07, -2.6931e-07,\n",
       "                      -1.8463e-08,  5.6368e-08, -3.5393e-07, -2.1127e-07, -1.1869e-07,\n",
       "                       8.8865e-07, -3.4443e-07,  1.7023e-07, -4.4581e-08, -2.2028e-07,\n",
       "                      -3.4277e-08,  3.6345e-07,  3.3264e-07, -3.1721e-07, -1.0421e-07,\n",
       "                      -2.1930e-07, -6.2709e-07,  3.7228e-07, -2.0274e-07,  5.3505e-07,\n",
       "                      -7.9445e-07, -9.1733e-08,  1.7960e-07,  1.8519e-07, -1.4701e-07,\n",
       "                       4.2194e-07,  7.0938e-07,  1.6250e-07,  5.1310e-08,  7.4510e-09,\n",
       "                       7.7255e-07,  2.5210e-09, -9.1375e-07,  3.2462e-07,  3.6285e-07,\n",
       "                       2.1503e-07, -7.1526e-08, -8.1828e-07,  1.9351e-07,  4.9521e-07,\n",
       "                       1.8208e-07, -1.3611e-07, -1.8239e-07, -2.1318e-08,  4.0281e-07,\n",
       "                      -1.3101e-07, -9.6379e-08, -2.2025e-07, -1.5349e-08,  2.1661e-07,\n",
       "                       1.7201e-07, -5.8415e-07,  6.7782e-08,  7.2697e-07,  6.0461e-07,\n",
       "                      -1.6923e-08,  5.7754e-07,  1.5504e-07, -2.3501e-07,  1.5885e-07,\n",
       "                      -2.5137e-08, -3.9250e-07, -5.9670e-07, -3.7158e-08, -2.8305e-07,\n",
       "                       2.2501e-07, -2.9136e-07,  5.2623e-07, -1.9886e-07,  3.5928e-07,\n",
       "                      -5.6267e-08, -1.3256e-06, -1.0016e-08, -3.0124e-08,  7.6378e-08,\n",
       "                       4.5150e-07, -1.0822e-07, -3.8042e-07,  4.6294e-07, -3.9584e-07,\n",
       "                       6.6075e-07,  2.4074e-07, -5.3756e-07,  2.4336e-07,  4.3332e-08,\n",
       "                       3.7744e-07,  6.9353e-07,  8.6947e-08, -7.0743e-07, -2.8765e-07,\n",
       "                      -1.9476e-07,  5.0228e-07,  2.4499e-07, -4.5460e-07, -5.4936e-08,\n",
       "                       4.7649e-07, -1.6355e-07,  4.1836e-07, -9.1585e-09, -4.9440e-08,\n",
       "                      -3.1708e-07, -6.9633e-07, -1.2833e-07, -3.4421e-07, -1.0659e-06,\n",
       "                       7.0982e-07, -5.7014e-07,  4.9460e-07,  1.2616e-07,  2.5165e-07,\n",
       "                       4.3596e-09, -1.0462e-07,  1.3048e-06,  1.1111e-07,  1.2693e-08,\n",
       "                      -2.4152e-07, -1.7087e-07,  1.6380e-07, -2.3743e-07, -4.7557e-07])),\n",
       "             ('batch1.running_mean',\n",
       "              tensor([ -71.4245, -420.9036,  -46.1286,  223.5482, -122.4517,   20.2931,\n",
       "                       -46.8378, -102.1536,  -19.5654, -141.7048,  117.5629,   96.9428,\n",
       "                         6.9780, -193.2507,  172.0240,  161.7203,  -63.7302,  212.5508,\n",
       "                       167.8592,  102.2379,   49.9099, -246.5458,   70.8116,  -38.4313,\n",
       "                       156.0647, -252.0714,   53.4705,  -12.5352,  163.1170,  -90.9351,\n",
       "                       -27.3040,  219.1492,   -1.4786,    6.6447, -144.1458,  351.4589,\n",
       "                       -28.6709,   57.6458,   33.8220,  138.5094,   -7.7925,  -61.9416,\n",
       "                      -147.5270,   64.6443,  165.4668,  -11.7237,  -25.1663,   38.1258,\n",
       "                      -136.6335,  246.3861,   66.0478, -194.1048, -185.7712, -373.9628,\n",
       "                        73.7102,   99.5011, -126.5287,   44.6722,   55.5093,   44.9917,\n",
       "                        92.3606,   95.0489,  -98.8674,  299.4138,   14.0389,   34.3440,\n",
       "                       131.6817, -116.2410,    1.6175,   92.9560,  -73.6595,   91.9449,\n",
       "                       238.9462, -249.2905,  -70.7183,  -79.8985,  123.5332, -126.0753,\n",
       "                        96.8284, -245.2943,  -84.7236,   -6.4040,  -48.3498,   33.2336,\n",
       "                       -98.1542,  147.8789,  145.2719,   59.5437,    9.1949,   77.9309,\n",
       "                       231.8698, -204.1005,  219.0235,  135.9311, -101.9260,  -36.2583,\n",
       "                        71.6275, -131.2645,  -63.4399,  -84.8726, -218.3801,  -82.2438,\n",
       "                       126.0986,   14.0828,  396.3401,   84.9292,   41.3911,   39.6142,\n",
       "                      -185.8728,  159.4208,  -91.6632, -110.2391,   58.6427, -182.3323,\n",
       "                      -156.6052, -159.5918,  -53.2225,  -74.1887,   61.0696,   50.3307,\n",
       "                      -101.2360,  107.5280,   20.4639, -152.3002,  194.6128,  -64.0682,\n",
       "                       105.1960,  -83.5854,  110.4939, -157.2104,   98.2860, -218.5776,\n",
       "                        53.9824,  165.7250, -111.3637,  101.5709,   47.6335,    3.5687,\n",
       "                       178.2872, -300.7117,   57.3945, -239.8000,  -50.0340, -232.8965,\n",
       "                       -22.7867,  -46.4921,  127.3601,  -80.4682, -150.7384,   73.0204,\n",
       "                        40.7072,  269.5099,  -36.5675,   80.5495,   25.8168,  339.6649,\n",
       "                      -162.5659,  116.9632, -104.4829, -207.0722, -182.8896, -233.7151,\n",
       "                       180.8494,  221.0352, -136.3491, -270.7057,  -93.7863,  -87.3578,\n",
       "                      -153.3874,  124.7541,   11.5553,   98.0454, -179.3101,   36.6535,\n",
       "                       -86.2099, -376.1590,  -24.7651,  -82.8154, -137.1344, -250.8530,\n",
       "                      -113.1095, -147.8262,    5.6586, -179.9021,   37.6759,  -15.1394,\n",
       "                        77.3657,  -44.5121,  -94.8111,    1.2718, -147.1349,  -54.1386,\n",
       "                       -40.9253,  -29.1527,  -55.4297,   37.2164,  -81.7054,   30.2831,\n",
       "                       -33.2307,  -36.7533,  239.3180,  -21.8360, -150.8216,   78.9945,\n",
       "                       -15.7100,  229.1275, -236.8140, -173.5160,  134.1170,  136.1405,\n",
       "                      -207.4214,  199.1929,  344.9757,   90.4704, -148.2899,   18.2783,\n",
       "                       120.7101,  270.7926,   12.1681,   80.5852, -123.8447,   76.9850,\n",
       "                       -37.2211,  -55.1168,  191.5424,   21.5255,  -29.5329,  -43.7734,\n",
       "                       156.4265,   99.4702, -186.4657,   97.3818,  -12.5115,  153.7397,\n",
       "                       136.0229])),\n",
       "             ('batch1.running_var',\n",
       "              tensor([ 1901.5339, 29384.7852,   833.7811, 20241.2344,  1594.7812,   628.9296,\n",
       "                       4056.9351,  1976.3019,   921.0661, 11111.7275,  2614.5986,  1483.3374,\n",
       "                       1051.0205, 10823.7109, 15585.5391,  2328.6433,  2718.8318, 11691.9795,\n",
       "                      11403.8467,   237.7396,  3576.4824, 11319.8662,   151.4570,   389.4008,\n",
       "                       4239.8662, 17891.3438,   512.4191,  4885.0684,  5168.4102,   836.3492,\n",
       "                        261.7002,  8560.1768,  1502.8774,   285.0096,  1839.4102, 28483.2988,\n",
       "                       1511.9512,  5629.5684,   595.4168,  6845.9497,   782.5181,   431.2849,\n",
       "                       1333.5442,   790.9769,  5741.8262,   212.9099,  1558.3756,   329.7323,\n",
       "                       1241.7059, 30069.0527,  5485.0425, 17375.4102,  6100.1558, 29044.8633,\n",
       "                       4181.2275,  2632.7834,  6433.8833,   551.4963,   191.8952,   364.8410,\n",
       "                       2908.8191,  1398.2416,  1193.4431, 21571.2930,   159.8046,   274.5213,\n",
       "                       5926.2510,   959.2490,  2059.5986,  1644.5580,   946.3815,  5570.6787,\n",
       "                      12756.5752, 25279.3926,   425.5314,  2137.1030,  2636.5884,  2014.7999,\n",
       "                       3230.3425, 23540.9434,  1889.7349,  1488.3149,   414.9088,  1791.6716,\n",
       "                        425.8028, 10639.7773, 11193.1494,  1023.5305,   804.0441,   352.1588,\n",
       "                       7705.8467, 10442.1172,  6974.0327,  6076.0073,  2328.6360,   197.9884,\n",
       "                       2233.7532,  9176.8311,  1044.4408,   276.3530,  4345.1851,  3557.2729,\n",
       "                       2025.5121,   334.8983, 30605.4473,  3496.3435,  4945.7749,   342.3553,\n",
       "                      16282.9600,  4174.2847,  1955.8987,  1594.6893,  2436.1443,  6406.7817,\n",
       "                       3773.3784,  4877.3047,  2153.1904,   587.8087,   169.0522,  1681.3741,\n",
       "                       1593.9755,  3057.6799,  2091.8459,  5685.6963, 13175.2754,   712.3559,\n",
       "                        798.1805,  3860.2290,  3911.3511, 12380.8809,  2555.2532, 22522.7637,\n",
       "                       1964.7686, 15111.1240,  4523.4575, 12548.6396,   121.9312,   606.1519,\n",
       "                       3897.5410, 29659.5664,   180.7437, 11214.4463,   314.5703, 13877.2461,\n",
       "                       3194.5137,   750.4100,  2101.7654,   793.9246,  4987.9668,  3217.4712,\n",
       "                       1042.9746, 21717.0625,   573.0024,   194.5020,  1461.7926, 28359.0898,\n",
       "                       7352.5068,  5854.2290,   366.1359, 11427.1631, 11268.2734, 18299.2285,\n",
       "                       5131.7900,  6801.8472,  2935.4749, 16202.5381,  3025.3689,   147.0926,\n",
       "                       4221.4443,  4295.4595,  1653.2083,   394.5089,  9618.3252,   381.1234,\n",
       "                       5583.7495, 34027.0117,   318.7695,   100.5379,  2610.0916, 15809.8379,\n",
       "                       6239.4873,  3269.1396,   497.3707,  5027.4678,  2218.6670,   729.8358,\n",
       "                        223.3061,   594.5488,  2188.3088,   345.9798, 12700.1201,  2456.5413,\n",
       "                       2574.4368,  2313.8772,   433.9837,   477.6509,   326.8803,   579.5475,\n",
       "                       5140.6812,  2556.6394,  8779.3506,   392.4349,  1852.4355,  3576.8357,\n",
       "                       3744.5115, 11327.5518,  8170.0894, 14560.0928,  1517.9791,  6020.9014,\n",
       "                      15871.1270,  9753.6875, 43339.4766,  2981.0955,  2106.1096,  2664.9534,\n",
       "                       5050.8091,  8899.5820,   265.1743,   555.0999,  1605.6274,   471.6273,\n",
       "                         96.5644,  1103.0408, 10212.0391,   294.3849,   583.6177,   578.2606,\n",
       "                       5474.4766,  8431.5498,  7512.1353,  6573.2075,  3637.5510,  4248.7637,\n",
       "                       8792.7480])),\n",
       "             ('batch1.num_batches_tracked', tensor(5000)),\n",
       "             ('linear2.weight',\n",
       "              tensor([[-0.0523, -0.0041,  0.0190,  ..., -0.0079,  0.0357, -0.0109],\n",
       "                      [ 0.0386,  0.0247, -0.0136,  ...,  0.0603,  0.0520,  0.0650],\n",
       "                      [ 0.0420,  0.0480,  0.0199,  ...,  0.0122, -0.0171, -0.0318],\n",
       "                      ...,\n",
       "                      [ 0.0274, -0.0364, -0.0294,  ..., -0.0474,  0.0072, -0.0242],\n",
       "                      [ 0.0126, -0.0458, -0.0175,  ...,  0.0190, -0.0386,  0.0582],\n",
       "                      [-0.0473,  0.0281, -0.0107,  ...,  0.0496,  0.0453, -0.0018]])),\n",
       "             ('linear2.bias',\n",
       "              tensor([ 0.0308, -0.0586,  0.0563,  0.0230, -0.0624, -0.0018,  0.0628, -0.0133,\n",
       "                      -0.0545, -0.0387,  0.0568,  0.0108,  0.0253,  0.0349, -0.0486,  0.0247,\n",
       "                       0.0414, -0.0362,  0.0060,  0.0086,  0.0179, -0.0113,  0.0034, -0.0268,\n",
       "                       0.0155,  0.0649, -0.0084,  0.0522,  0.0562,  0.0047, -0.0227,  0.0572,\n",
       "                      -0.0619,  0.0629,  0.0172, -0.0277,  0.0551,  0.0604, -0.0550, -0.0570,\n",
       "                       0.0085,  0.0401,  0.0563, -0.0396,  0.0106, -0.0178,  0.0459, -0.0420,\n",
       "                       0.0072,  0.0639, -0.0090, -0.0435, -0.0622, -0.0358,  0.0555, -0.0334,\n",
       "                      -0.0448, -0.0505,  0.0583, -0.0213,  0.0375, -0.0189, -0.0315, -0.0186,\n",
       "                      -0.0468,  0.0201, -0.0576, -0.0309, -0.0501, -0.0011, -0.0514, -0.0452,\n",
       "                       0.0591, -0.0266, -0.0276, -0.0409,  0.0311, -0.0343,  0.0133, -0.0589,\n",
       "                       0.0500,  0.0511,  0.0633, -0.0499, -0.0615,  0.0369, -0.0239, -0.0216,\n",
       "                      -0.0157,  0.0212, -0.0321, -0.0387, -0.0241,  0.0322, -0.0397,  0.0010,\n",
       "                       0.0224, -0.0018, -0.0298,  0.0125])),\n",
       "             ('batch2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('batch2.bias',\n",
       "              tensor([-2.2762e-07,  7.0525e-07, -9.1940e-07, -6.5288e-08,  1.2427e-07,\n",
       "                       2.5094e-07, -6.8171e-07, -2.9247e-07,  1.8217e-07, -9.7732e-07,\n",
       "                       5.4765e-07,  7.3377e-08, -9.2969e-07,  1.0735e-07,  1.1834e-07,\n",
       "                      -5.7164e-07, -3.6258e-07,  3.0997e-07,  5.2821e-07, -3.2010e-07,\n",
       "                       4.0551e-07, -8.4152e-07,  8.0577e-07,  2.2135e-07,  1.4701e-07,\n",
       "                      -4.7796e-07,  5.2044e-07,  2.2653e-09, -7.0618e-07,  1.4517e-07,\n",
       "                       1.1100e-07, -4.7671e-07,  1.6476e-08,  7.9813e-07,  1.0975e-06,\n",
       "                      -2.4273e-07,  1.0612e-07,  1.7355e-07,  1.0534e-07,  1.0785e-07,\n",
       "                      -2.7264e-07,  9.1934e-07, -3.7627e-07, -3.6769e-07,  1.5808e-06,\n",
       "                      -1.0943e-06,  5.2885e-07, -8.0339e-09, -5.7482e-08, -6.6237e-07,\n",
       "                       7.3818e-07, -3.2267e-07, -7.3434e-07, -1.6039e-07,  2.9979e-07,\n",
       "                       1.4976e-07, -1.3301e-07, -4.0172e-07,  3.1610e-07,  9.6887e-07,\n",
       "                      -6.2470e-07,  1.6119e-07,  5.4813e-08,  3.9199e-08,  6.8262e-07,\n",
       "                       4.0209e-07,  2.2083e-07,  9.4777e-08, -1.4090e-07, -3.2555e-07,\n",
       "                       6.8725e-07,  1.8241e-07, -3.0460e-07, -2.0503e-08, -2.2953e-07,\n",
       "                      -3.4481e-07,  2.0229e-08,  9.2232e-08,  2.1947e-07,  1.6924e-07,\n",
       "                      -3.9328e-07,  4.7698e-07, -1.1541e-06,  9.2050e-08,  2.2298e-07,\n",
       "                      -3.4624e-07, -5.2339e-07,  6.2884e-07,  1.7464e-07, -1.0236e-06,\n",
       "                       3.9415e-07, -6.1046e-07,  8.0113e-07,  8.6029e-07,  1.5246e-07,\n",
       "                       5.0858e-07, -3.1620e-07, -1.5958e-06, -6.5552e-07, -4.9580e-08])),\n",
       "             ('batch2.running_mean',\n",
       "              tensor([-0.5063,  0.0658,  0.1237, -0.2722, -0.1693,  0.4204,  0.0354, -0.1981,\n",
       "                       0.0603,  0.0353,  0.4993, -0.1678, -0.1369, -0.0683, -0.3539, -0.3272,\n",
       "                      -0.5471, -0.0442,  0.4870, -0.1201, -0.3743,  0.1545,  0.1691,  0.1661,\n",
       "                      -0.1568,  0.1647, -0.1257, -0.1309,  0.0755,  0.1964,  0.8460, -0.1642,\n",
       "                       0.1243, -0.1022,  0.1377,  0.5173,  0.0584,  0.1772, -0.0786, -0.1174,\n",
       "                      -0.0328,  0.1575,  0.4634, -0.0268, -0.2937,  0.0306,  0.1827, -0.0892,\n",
       "                      -0.3074,  0.0940,  0.1715,  0.0555,  0.2872, -0.1878,  0.0462,  0.2167,\n",
       "                       0.0288, -0.0958,  0.4354, -0.2325, -0.1476, -0.1663, -0.1073, -0.2413,\n",
       "                      -0.0761,  0.1487, -0.1535,  0.1857,  0.0279,  0.0153,  0.0416, -0.3076,\n",
       "                      -0.0780,  0.1264, -0.1747, -0.0647, -0.1510,  0.1537, -0.0833, -0.0169,\n",
       "                       0.3235, -0.0491, -0.1677,  0.1098,  0.1625,  0.0073, -0.0135, -0.0887,\n",
       "                       0.3240,  0.1346, -0.0234, -0.0886, -0.0700, -0.1356, -0.3215,  0.2065,\n",
       "                       0.4089, -0.1794, -0.1197,  0.4325])),\n",
       "             ('batch2.running_var',\n",
       "              tensor([0.1555, 0.0646, 0.0622, 0.1549, 0.0977, 0.1656, 0.1333, 0.3693, 0.0387,\n",
       "                      0.0402, 0.0743, 0.0330, 0.1175, 0.0152, 0.1383, 0.0871, 0.1100, 0.0408,\n",
       "                      0.3082, 0.0931, 0.1293, 0.0654, 0.0573, 0.1574, 0.1291, 0.3217, 0.0479,\n",
       "                      0.3724, 0.0598, 0.0520, 0.2079, 0.0281, 0.0685, 0.0269, 0.0317, 0.2308,\n",
       "                      0.1628, 0.1359, 0.0321, 0.2704, 0.0821, 0.0531, 0.0644, 0.0240, 0.0491,\n",
       "                      0.0933, 0.1392, 0.0441, 0.0734, 0.3282, 0.0340, 0.0569, 0.0976, 0.0308,\n",
       "                      0.1180, 0.0851, 0.1156, 0.0842, 0.0730, 0.0382, 0.2386, 0.1952, 0.0535,\n",
       "                      0.0787, 0.1995, 0.0538, 0.0555, 0.2117, 0.0473, 0.1553, 0.0559, 0.0232,\n",
       "                      0.0364, 0.1463, 0.1040, 0.0279, 0.1569, 0.0468, 0.1239, 0.3711, 0.0791,\n",
       "                      0.0392, 0.0500, 0.0923, 0.1115, 0.0644, 0.0438, 0.0628, 0.0614, 0.0317,\n",
       "                      0.0459, 0.0809, 0.0841, 0.0854, 0.0597, 0.3887, 0.0561, 0.0531, 0.3205,\n",
       "                      0.2006])),\n",
       "             ('batch2.num_batches_tracked', tensor(5000)),\n",
       "             ('linear3.weight',\n",
       "              tensor([[-0.0637, -0.0345, -0.0874,  ..., -0.0941, -0.0619,  0.0440],\n",
       "                      [-0.0242,  0.0799,  0.0206,  ..., -0.0790,  0.0136, -0.0173],\n",
       "                      [ 0.0671, -0.0972, -0.0715,  ...,  0.0137,  0.0875, -0.0843],\n",
       "                      ...,\n",
       "                      [-0.0272,  0.0820, -0.0828,  ...,  0.0803, -0.0081, -0.0968],\n",
       "                      [ 0.0828, -0.0695, -0.0766,  ..., -0.0064, -0.0443, -0.0491],\n",
       "                      [-0.0101,  0.0197,  0.0234,  ...,  0.0445, -0.0183, -0.0813]])),\n",
       "             ('linear3.bias',\n",
       "              tensor([ 0.0329, -0.0381,  0.0676,  0.0020, -0.0063,  0.0267, -0.0700,  0.0649,\n",
       "                       0.0990, -0.0216, -0.0033,  0.0759,  0.0578,  0.0269,  0.0650, -0.0680,\n",
       "                      -0.0332,  0.0114,  0.0552, -0.0195, -0.0611, -0.0011, -0.0297, -0.0611,\n",
       "                      -0.0802,  0.0234,  0.0973, -0.0879,  0.0825,  0.0458,  0.0351,  0.0202,\n",
       "                       0.0146,  0.0589, -0.0995,  0.0316, -0.0007,  0.0948, -0.0640,  0.0351])),\n",
       "             ('batch3.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('batch3.bias',\n",
       "              tensor([-2.4884e-07,  1.5586e-06, -2.4431e-06, -2.4190e-06,  2.0924e-06,\n",
       "                      -1.2151e-06, -7.9744e-07, -7.2943e-07, -1.4286e-06, -2.9294e-06,\n",
       "                      -1.3046e-06, -2.0203e-06, -1.7666e-07, -2.5090e-06, -5.6352e-07,\n",
       "                       1.1448e-06, -1.1247e-07,  5.8356e-07, -3.9519e-06, -1.9218e-06,\n",
       "                      -5.3049e-08,  6.1213e-07,  3.5382e-06, -3.4808e-06, -9.8321e-07,\n",
       "                      -6.8899e-07, -1.4771e-06,  2.5334e-06, -3.0452e-06, -5.1514e-07,\n",
       "                      -2.5187e-06,  1.3302e-06,  3.1339e-06, -1.6064e-07,  7.5661e-07,\n",
       "                      -1.1919e-08, -2.5979e-07,  2.4924e-06,  3.0473e-06, -8.1018e-07])),\n",
       "             ('batch3.running_mean',\n",
       "              tensor([-9.3500e-04, -5.9902e-01,  4.1715e-01,  1.4749e-01,  2.3094e-01,\n",
       "                       1.8619e-01, -1.5214e-01,  1.2994e-01,  1.9057e-01, -2.4725e-01,\n",
       "                      -3.3799e-01, -8.6757e-02,  2.1966e-01, -1.1548e-01,  4.3301e-01,\n",
       "                      -2.5757e-01,  2.4456e-01, -8.2611e-02, -1.2240e-01,  2.9056e-01,\n",
       "                       1.3048e-01,  1.5789e-02, -2.0351e-01, -1.4625e-01, -1.0354e-01,\n",
       "                      -2.2461e-01,  9.9927e-02, -4.0602e-01,  3.1608e-02,  1.2915e-01,\n",
       "                       2.3557e-04,  1.3078e-01, -1.0112e-01, -2.8686e-01, -3.8577e-01,\n",
       "                      -1.1757e-01,  2.2189e-01, -3.1789e-01,  2.6991e-01, -4.5076e-02])),\n",
       "             ('batch3.running_var',\n",
       "              tensor([0.1214, 0.1216, 0.1593, 0.1219, 0.1146, 0.0779, 0.0755, 0.1295, 0.1719,\n",
       "                      0.3926, 0.0964, 0.0908, 0.0785, 0.0958, 0.1433, 0.0580, 0.0805, 0.0412,\n",
       "                      0.0595, 0.1150, 0.0539, 0.1278, 0.0581, 0.0843, 0.0899, 0.1049, 0.0875,\n",
       "                      0.0700, 0.0385, 0.0661, 0.1911, 0.1702, 0.2593, 0.0891, 0.0638, 0.0634,\n",
       "                      0.4080, 0.1203, 0.3464, 0.0887])),\n",
       "             ('batch3.num_batches_tracked', tensor(5000)),\n",
       "             ('linear4.weight',\n",
       "              tensor([[-0.0120,  0.0592, -0.1182,  0.1282,  0.1475, -0.1007,  0.1400, -0.0460,\n",
       "                        0.1054, -0.1499, -0.0888, -0.1189, -0.0085, -0.1081,  0.0509,  0.0424,\n",
       "                       -0.0540,  0.0227, -0.1324,  0.1330, -0.0533, -0.1015,  0.1270, -0.1076,\n",
       "                        0.0923, -0.0847, -0.0671,  0.0955, -0.1404, -0.0803, -0.1079,  0.1525,\n",
       "                        0.1243, -0.0075,  0.0585,  0.0189, -0.0153,  0.1343,  0.1300, -0.0477]])),\n",
       "             ('linear4.bias', tensor([-0.1269]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we load our saved model\n",
    "#We define our hyperparameters\n",
    "model1 = neural_net(605, 235, 100, 40, 1)\n",
    "optimizer = optim.SGD(model1.parameters(), lr = 0.000001)\n",
    "checkpoint = torch.load(PATH)  # load the saved checkpoint\n",
    "\n",
    "# load the saved information back\n",
    "model1.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neural_net(\n",
       "  (relu): LeakyReLU(negative_slope=0.01)\n",
       "  (sig): Sigmoid()\n",
       "  (linear1): Linear(in_features=605, out_features=235, bias=True)\n",
       "  (batch1): BatchNorm1d(235, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear2): Linear(in_features=235, out_features=100, bias=True)\n",
       "  (batch2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear3): Linear(in_features=100, out_features=40, bias=True)\n",
       "  (batch3): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear4): Linear(in_features=40, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear1.weight',\n",
       "              tensor([[ 0.0093, -0.0097,  0.0111,  ..., -0.0032,  0.0351,  0.0035],\n",
       "                      [-0.0149, -0.0063, -0.0359,  ...,  0.0256,  0.0210,  0.0283],\n",
       "                      [-0.0061,  0.0085,  0.0003,  ..., -0.0248, -0.0059, -0.0227],\n",
       "                      ...,\n",
       "                      [ 0.0084,  0.0354,  0.0326,  ...,  0.0044, -0.0214,  0.0067],\n",
       "                      [-0.0034, -0.0237,  0.0043,  ...,  0.0362,  0.0141,  0.0185],\n",
       "                      [-0.0162,  0.0178, -0.0222,  ..., -0.0211, -0.0069,  0.0194]])),\n",
       "             ('linear1.bias',\n",
       "              tensor([-0.0261,  0.0360, -0.0307, -0.0211, -0.0145, -0.0137,  0.0054, -0.0342,\n",
       "                      -0.0303,  0.0081,  0.0090,  0.0391,  0.0246, -0.0144, -0.0052, -0.0096,\n",
       "                      -0.0098,  0.0393, -0.0057,  0.0137, -0.0088,  0.0092,  0.0126,  0.0022,\n",
       "                      -0.0275, -0.0284, -0.0109,  0.0014,  0.0117,  0.0254, -0.0023,  0.0243,\n",
       "                      -0.0154,  0.0004, -0.0200,  0.0145, -0.0197, -0.0241, -0.0366, -0.0284,\n",
       "                       0.0383, -0.0193, -0.0148, -0.0243,  0.0391,  0.0231, -0.0123,  0.0109,\n",
       "                       0.0188, -0.0355,  0.0325,  0.0357, -0.0060,  0.0318, -0.0036, -0.0192,\n",
       "                       0.0250, -0.0187,  0.0334, -0.0072, -0.0202,  0.0403,  0.0127, -0.0319,\n",
       "                      -0.0141,  0.0134, -0.0314, -0.0386, -0.0066,  0.0381,  0.0301, -0.0182,\n",
       "                       0.0229, -0.0260, -0.0064,  0.0062, -0.0223,  0.0168, -0.0349, -0.0066,\n",
       "                      -0.0093,  0.0102, -0.0101,  0.0052, -0.0203, -0.0286, -0.0188, -0.0098,\n",
       "                      -0.0328, -0.0103,  0.0324,  0.0301, -0.0306,  0.0074, -0.0070,  0.0143,\n",
       "                      -0.0114, -0.0385, -0.0390, -0.0309,  0.0245, -0.0082, -0.0142, -0.0291,\n",
       "                       0.0290,  0.0170,  0.0211,  0.0060,  0.0218,  0.0316, -0.0149,  0.0278,\n",
       "                       0.0146,  0.0052, -0.0049,  0.0157,  0.0330, -0.0075, -0.0075,  0.0378,\n",
       "                       0.0373, -0.0189,  0.0056, -0.0136, -0.0402, -0.0151,  0.0019, -0.0327,\n",
       "                      -0.0233, -0.0293, -0.0098, -0.0104, -0.0171,  0.0319, -0.0350, -0.0077,\n",
       "                       0.0333,  0.0063, -0.0251,  0.0277, -0.0318, -0.0153,  0.0321,  0.0251,\n",
       "                       0.0033,  0.0125, -0.0066, -0.0339,  0.0165, -0.0263,  0.0226,  0.0147,\n",
       "                      -0.0124,  0.0057,  0.0106,  0.0179, -0.0055,  0.0126, -0.0379,  0.0049,\n",
       "                       0.0130, -0.0364, -0.0165,  0.0011, -0.0003, -0.0239, -0.0351,  0.0281,\n",
       "                      -0.0343, -0.0017,  0.0334, -0.0088, -0.0085, -0.0278,  0.0271,  0.0018,\n",
       "                      -0.0307,  0.0053,  0.0359,  0.0246, -0.0127, -0.0204,  0.0246,  0.0105,\n",
       "                       0.0238, -0.0181, -0.0220,  0.0236,  0.0024,  0.0111,  0.0225,  0.0400,\n",
       "                       0.0164,  0.0231,  0.0312, -0.0292,  0.0393, -0.0398,  0.0063, -0.0230,\n",
       "                       0.0071, -0.0163, -0.0094, -0.0039,  0.0269,  0.0249,  0.0062,  0.0132,\n",
       "                       0.0198,  0.0260, -0.0302,  0.0299,  0.0068,  0.0027,  0.0210,  0.0121,\n",
       "                      -0.0200,  0.0050, -0.0021, -0.0272,  0.0244, -0.0197,  0.0334,  0.0232,\n",
       "                      -0.0265,  0.0344, -0.0190, -0.0082, -0.0287,  0.0107, -0.0206,  0.0090,\n",
       "                       0.0208, -0.0070, -0.0014])),\n",
       "             ('batch1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1.])),\n",
       "             ('batch1.bias',\n",
       "              tensor([-8.7568e-08,  8.1622e-07,  5.1634e-07, -1.7802e-07,  4.2740e-08,\n",
       "                      -5.9221e-07,  1.0463e-07,  7.3632e-07,  3.6483e-07,  2.2382e-07,\n",
       "                      -1.0244e-07,  2.7068e-07, -3.5032e-07, -3.9866e-07, -5.0464e-07,\n",
       "                       6.9597e-08, -9.2440e-07,  2.1746e-07, -3.7782e-07,  6.2752e-07,\n",
       "                       2.1400e-08, -2.8257e-07,  3.5599e-07, -5.1780e-07,  2.3759e-07,\n",
       "                       1.3862e-09, -9.7663e-07, -3.8959e-07,  3.6905e-08,  5.7845e-07,\n",
       "                      -5.4163e-07,  1.7199e-07,  2.6558e-07,  5.4187e-08, -4.8316e-08,\n",
       "                       4.0875e-07, -4.2005e-07, -4.6824e-07,  1.6807e-07,  4.8622e-07,\n",
       "                       2.4506e-07,  3.2887e-07,  1.8370e-07,  9.3898e-07,  2.2714e-07,\n",
       "                      -6.1343e-07, -7.7130e-08,  1.3321e-07, -1.8196e-07, -6.6288e-08,\n",
       "                      -4.8790e-07, -4.2376e-07,  3.9530e-07,  2.7368e-07, -1.4716e-07,\n",
       "                      -1.8556e-07,  2.1011e-07,  3.8509e-07, -4.4793e-08, -1.1879e-07,\n",
       "                       5.3437e-07,  6.0029e-07, -6.7958e-07,  5.8076e-09, -2.4536e-07,\n",
       "                       8.1789e-08,  3.6566e-08,  1.0316e-06, -2.7524e-07, -5.3320e-07,\n",
       "                       5.8145e-07, -1.7958e-08,  8.6857e-07, -3.0206e-07,  1.1160e-06,\n",
       "                      -3.6327e-07, -7.5124e-08,  1.6929e-07, -1.3880e-06, -2.6638e-07,\n",
       "                      -2.8781e-07,  1.2476e-07, -2.5858e-07, -8.3680e-07,  2.1668e-07,\n",
       "                      -3.0268e-07,  2.9039e-07, -1.7007e-07,  2.7284e-07, -5.5291e-07,\n",
       "                      -4.2741e-07, -4.9795e-07,  3.6680e-08, -9.2359e-08,  2.9317e-07,\n",
       "                      -2.4040e-07, -1.4360e-07,  7.4339e-07,  5.2832e-08,  2.5201e-07,\n",
       "                       4.5343e-07, -1.6013e-08,  8.5673e-08, -2.7679e-07, -2.6013e-07,\n",
       "                      -7.6917e-07,  7.6982e-08, -6.5104e-07,  5.4510e-07,  4.4089e-07,\n",
       "                      -3.9773e-07, -1.6111e-07,  3.7309e-07,  1.5943e-07, -2.6931e-07,\n",
       "                      -1.8463e-08,  5.6368e-08, -3.5393e-07, -2.1127e-07, -1.1869e-07,\n",
       "                       8.8865e-07, -3.4443e-07,  1.7023e-07, -4.4581e-08, -2.2028e-07,\n",
       "                      -3.4277e-08,  3.6345e-07,  3.3264e-07, -3.1721e-07, -1.0421e-07,\n",
       "                      -2.1930e-07, -6.2709e-07,  3.7228e-07, -2.0274e-07,  5.3505e-07,\n",
       "                      -7.9445e-07, -9.1733e-08,  1.7960e-07,  1.8519e-07, -1.4701e-07,\n",
       "                       4.2194e-07,  7.0938e-07,  1.6250e-07,  5.1310e-08,  7.4510e-09,\n",
       "                       7.7255e-07,  2.5210e-09, -9.1375e-07,  3.2462e-07,  3.6285e-07,\n",
       "                       2.1503e-07, -7.1526e-08, -8.1828e-07,  1.9351e-07,  4.9521e-07,\n",
       "                       1.8208e-07, -1.3611e-07, -1.8239e-07, -2.1318e-08,  4.0281e-07,\n",
       "                      -1.3101e-07, -9.6379e-08, -2.2025e-07, -1.5349e-08,  2.1661e-07,\n",
       "                       1.7201e-07, -5.8415e-07,  6.7782e-08,  7.2697e-07,  6.0461e-07,\n",
       "                      -1.6923e-08,  5.7754e-07,  1.5504e-07, -2.3501e-07,  1.5885e-07,\n",
       "                      -2.5137e-08, -3.9250e-07, -5.9670e-07, -3.7158e-08, -2.8305e-07,\n",
       "                       2.2501e-07, -2.9136e-07,  5.2623e-07, -1.9886e-07,  3.5928e-07,\n",
       "                      -5.6267e-08, -1.3256e-06, -1.0016e-08, -3.0124e-08,  7.6378e-08,\n",
       "                       4.5150e-07, -1.0822e-07, -3.8042e-07,  4.6294e-07, -3.9584e-07,\n",
       "                       6.6075e-07,  2.4074e-07, -5.3756e-07,  2.4336e-07,  4.3332e-08,\n",
       "                       3.7744e-07,  6.9353e-07,  8.6947e-08, -7.0743e-07, -2.8765e-07,\n",
       "                      -1.9476e-07,  5.0228e-07,  2.4499e-07, -4.5460e-07, -5.4936e-08,\n",
       "                       4.7649e-07, -1.6355e-07,  4.1836e-07, -9.1585e-09, -4.9440e-08,\n",
       "                      -3.1708e-07, -6.9633e-07, -1.2833e-07, -3.4421e-07, -1.0659e-06,\n",
       "                       7.0982e-07, -5.7014e-07,  4.9460e-07,  1.2616e-07,  2.5165e-07,\n",
       "                       4.3596e-09, -1.0462e-07,  1.3048e-06,  1.1111e-07,  1.2693e-08,\n",
       "                      -2.4152e-07, -1.7087e-07,  1.6380e-07, -2.3743e-07, -4.7557e-07])),\n",
       "             ('batch1.running_mean',\n",
       "              tensor([ -71.4245, -420.9036,  -46.1286,  223.5482, -122.4517,   20.2931,\n",
       "                       -46.8378, -102.1536,  -19.5654, -141.7048,  117.5629,   96.9428,\n",
       "                         6.9780, -193.2507,  172.0240,  161.7203,  -63.7302,  212.5508,\n",
       "                       167.8592,  102.2379,   49.9099, -246.5458,   70.8116,  -38.4313,\n",
       "                       156.0647, -252.0714,   53.4705,  -12.5352,  163.1170,  -90.9351,\n",
       "                       -27.3040,  219.1492,   -1.4786,    6.6447, -144.1458,  351.4589,\n",
       "                       -28.6709,   57.6458,   33.8220,  138.5094,   -7.7925,  -61.9416,\n",
       "                      -147.5270,   64.6443,  165.4668,  -11.7237,  -25.1663,   38.1258,\n",
       "                      -136.6335,  246.3861,   66.0478, -194.1048, -185.7712, -373.9628,\n",
       "                        73.7102,   99.5011, -126.5287,   44.6722,   55.5093,   44.9917,\n",
       "                        92.3606,   95.0489,  -98.8674,  299.4138,   14.0389,   34.3440,\n",
       "                       131.6817, -116.2410,    1.6175,   92.9560,  -73.6595,   91.9449,\n",
       "                       238.9462, -249.2905,  -70.7183,  -79.8985,  123.5332, -126.0753,\n",
       "                        96.8284, -245.2943,  -84.7236,   -6.4040,  -48.3498,   33.2336,\n",
       "                       -98.1542,  147.8789,  145.2719,   59.5437,    9.1949,   77.9309,\n",
       "                       231.8698, -204.1005,  219.0235,  135.9311, -101.9260,  -36.2583,\n",
       "                        71.6275, -131.2645,  -63.4399,  -84.8726, -218.3801,  -82.2438,\n",
       "                       126.0986,   14.0828,  396.3401,   84.9292,   41.3911,   39.6142,\n",
       "                      -185.8728,  159.4208,  -91.6632, -110.2391,   58.6427, -182.3323,\n",
       "                      -156.6052, -159.5918,  -53.2225,  -74.1887,   61.0696,   50.3307,\n",
       "                      -101.2360,  107.5280,   20.4639, -152.3002,  194.6128,  -64.0682,\n",
       "                       105.1960,  -83.5854,  110.4939, -157.2104,   98.2860, -218.5776,\n",
       "                        53.9824,  165.7250, -111.3637,  101.5709,   47.6335,    3.5687,\n",
       "                       178.2872, -300.7117,   57.3945, -239.8000,  -50.0340, -232.8965,\n",
       "                       -22.7867,  -46.4921,  127.3601,  -80.4682, -150.7384,   73.0204,\n",
       "                        40.7072,  269.5099,  -36.5675,   80.5495,   25.8168,  339.6649,\n",
       "                      -162.5659,  116.9632, -104.4829, -207.0722, -182.8896, -233.7151,\n",
       "                       180.8494,  221.0352, -136.3491, -270.7057,  -93.7863,  -87.3578,\n",
       "                      -153.3874,  124.7541,   11.5553,   98.0454, -179.3101,   36.6535,\n",
       "                       -86.2099, -376.1590,  -24.7651,  -82.8154, -137.1344, -250.8530,\n",
       "                      -113.1095, -147.8262,    5.6586, -179.9021,   37.6759,  -15.1394,\n",
       "                        77.3657,  -44.5121,  -94.8111,    1.2718, -147.1349,  -54.1386,\n",
       "                       -40.9253,  -29.1527,  -55.4297,   37.2164,  -81.7054,   30.2831,\n",
       "                       -33.2307,  -36.7533,  239.3180,  -21.8360, -150.8216,   78.9945,\n",
       "                       -15.7100,  229.1275, -236.8140, -173.5160,  134.1170,  136.1405,\n",
       "                      -207.4214,  199.1929,  344.9757,   90.4704, -148.2899,   18.2783,\n",
       "                       120.7101,  270.7926,   12.1681,   80.5852, -123.8447,   76.9850,\n",
       "                       -37.2211,  -55.1168,  191.5424,   21.5255,  -29.5329,  -43.7734,\n",
       "                       156.4265,   99.4702, -186.4657,   97.3818,  -12.5115,  153.7397,\n",
       "                       136.0229])),\n",
       "             ('batch1.running_var',\n",
       "              tensor([ 1901.5339, 29384.7852,   833.7811, 20241.2344,  1594.7812,   628.9296,\n",
       "                       4056.9351,  1976.3019,   921.0661, 11111.7275,  2614.5986,  1483.3374,\n",
       "                       1051.0205, 10823.7109, 15585.5391,  2328.6433,  2718.8318, 11691.9795,\n",
       "                      11403.8467,   237.7396,  3576.4824, 11319.8662,   151.4570,   389.4008,\n",
       "                       4239.8662, 17891.3438,   512.4191,  4885.0684,  5168.4102,   836.3492,\n",
       "                        261.7002,  8560.1768,  1502.8774,   285.0096,  1839.4102, 28483.2988,\n",
       "                       1511.9512,  5629.5684,   595.4168,  6845.9497,   782.5181,   431.2849,\n",
       "                       1333.5442,   790.9769,  5741.8262,   212.9099,  1558.3756,   329.7323,\n",
       "                       1241.7059, 30069.0527,  5485.0425, 17375.4102,  6100.1558, 29044.8633,\n",
       "                       4181.2275,  2632.7834,  6433.8833,   551.4963,   191.8952,   364.8410,\n",
       "                       2908.8191,  1398.2416,  1193.4431, 21571.2930,   159.8046,   274.5213,\n",
       "                       5926.2510,   959.2490,  2059.5986,  1644.5580,   946.3815,  5570.6787,\n",
       "                      12756.5752, 25279.3926,   425.5314,  2137.1030,  2636.5884,  2014.7999,\n",
       "                       3230.3425, 23540.9434,  1889.7349,  1488.3149,   414.9088,  1791.6716,\n",
       "                        425.8028, 10639.7773, 11193.1494,  1023.5305,   804.0441,   352.1588,\n",
       "                       7705.8467, 10442.1172,  6974.0327,  6076.0073,  2328.6360,   197.9884,\n",
       "                       2233.7532,  9176.8311,  1044.4408,   276.3530,  4345.1851,  3557.2729,\n",
       "                       2025.5121,   334.8983, 30605.4473,  3496.3435,  4945.7749,   342.3553,\n",
       "                      16282.9600,  4174.2847,  1955.8987,  1594.6893,  2436.1443,  6406.7817,\n",
       "                       3773.3784,  4877.3047,  2153.1904,   587.8087,   169.0522,  1681.3741,\n",
       "                       1593.9755,  3057.6799,  2091.8459,  5685.6963, 13175.2754,   712.3559,\n",
       "                        798.1805,  3860.2290,  3911.3511, 12380.8809,  2555.2532, 22522.7637,\n",
       "                       1964.7686, 15111.1240,  4523.4575, 12548.6396,   121.9312,   606.1519,\n",
       "                       3897.5410, 29659.5664,   180.7437, 11214.4463,   314.5703, 13877.2461,\n",
       "                       3194.5137,   750.4100,  2101.7654,   793.9246,  4987.9668,  3217.4712,\n",
       "                       1042.9746, 21717.0625,   573.0024,   194.5020,  1461.7926, 28359.0898,\n",
       "                       7352.5068,  5854.2290,   366.1359, 11427.1631, 11268.2734, 18299.2285,\n",
       "                       5131.7900,  6801.8472,  2935.4749, 16202.5381,  3025.3689,   147.0926,\n",
       "                       4221.4443,  4295.4595,  1653.2083,   394.5089,  9618.3252,   381.1234,\n",
       "                       5583.7495, 34027.0117,   318.7695,   100.5379,  2610.0916, 15809.8379,\n",
       "                       6239.4873,  3269.1396,   497.3707,  5027.4678,  2218.6670,   729.8358,\n",
       "                        223.3061,   594.5488,  2188.3088,   345.9798, 12700.1201,  2456.5413,\n",
       "                       2574.4368,  2313.8772,   433.9837,   477.6509,   326.8803,   579.5475,\n",
       "                       5140.6812,  2556.6394,  8779.3506,   392.4349,  1852.4355,  3576.8357,\n",
       "                       3744.5115, 11327.5518,  8170.0894, 14560.0928,  1517.9791,  6020.9014,\n",
       "                      15871.1270,  9753.6875, 43339.4766,  2981.0955,  2106.1096,  2664.9534,\n",
       "                       5050.8091,  8899.5820,   265.1743,   555.0999,  1605.6274,   471.6273,\n",
       "                         96.5644,  1103.0408, 10212.0391,   294.3849,   583.6177,   578.2606,\n",
       "                       5474.4766,  8431.5498,  7512.1353,  6573.2075,  3637.5510,  4248.7637,\n",
       "                       8792.7480])),\n",
       "             ('batch1.num_batches_tracked', tensor(5000)),\n",
       "             ('linear2.weight',\n",
       "              tensor([[-0.0523, -0.0041,  0.0190,  ..., -0.0079,  0.0357, -0.0109],\n",
       "                      [ 0.0386,  0.0247, -0.0136,  ...,  0.0603,  0.0520,  0.0650],\n",
       "                      [ 0.0420,  0.0480,  0.0199,  ...,  0.0122, -0.0171, -0.0318],\n",
       "                      ...,\n",
       "                      [ 0.0274, -0.0364, -0.0294,  ..., -0.0474,  0.0072, -0.0242],\n",
       "                      [ 0.0126, -0.0458, -0.0175,  ...,  0.0190, -0.0386,  0.0582],\n",
       "                      [-0.0473,  0.0281, -0.0107,  ...,  0.0496,  0.0453, -0.0018]])),\n",
       "             ('linear2.bias',\n",
       "              tensor([ 0.0308, -0.0586,  0.0563,  0.0230, -0.0624, -0.0018,  0.0628, -0.0133,\n",
       "                      -0.0545, -0.0387,  0.0568,  0.0108,  0.0253,  0.0349, -0.0486,  0.0247,\n",
       "                       0.0414, -0.0362,  0.0060,  0.0086,  0.0179, -0.0113,  0.0034, -0.0268,\n",
       "                       0.0155,  0.0649, -0.0084,  0.0522,  0.0562,  0.0047, -0.0227,  0.0572,\n",
       "                      -0.0619,  0.0629,  0.0172, -0.0277,  0.0551,  0.0604, -0.0550, -0.0570,\n",
       "                       0.0085,  0.0401,  0.0563, -0.0396,  0.0106, -0.0178,  0.0459, -0.0420,\n",
       "                       0.0072,  0.0639, -0.0090, -0.0435, -0.0622, -0.0358,  0.0555, -0.0334,\n",
       "                      -0.0448, -0.0505,  0.0583, -0.0213,  0.0375, -0.0189, -0.0315, -0.0186,\n",
       "                      -0.0468,  0.0201, -0.0576, -0.0309, -0.0501, -0.0011, -0.0514, -0.0452,\n",
       "                       0.0591, -0.0266, -0.0276, -0.0409,  0.0311, -0.0343,  0.0133, -0.0589,\n",
       "                       0.0500,  0.0511,  0.0633, -0.0499, -0.0615,  0.0369, -0.0239, -0.0216,\n",
       "                      -0.0157,  0.0212, -0.0321, -0.0387, -0.0241,  0.0322, -0.0397,  0.0010,\n",
       "                       0.0224, -0.0018, -0.0298,  0.0125])),\n",
       "             ('batch2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('batch2.bias',\n",
       "              tensor([-2.2762e-07,  7.0525e-07, -9.1940e-07, -6.5288e-08,  1.2427e-07,\n",
       "                       2.5094e-07, -6.8171e-07, -2.9247e-07,  1.8217e-07, -9.7732e-07,\n",
       "                       5.4765e-07,  7.3377e-08, -9.2969e-07,  1.0735e-07,  1.1834e-07,\n",
       "                      -5.7164e-07, -3.6258e-07,  3.0997e-07,  5.2821e-07, -3.2010e-07,\n",
       "                       4.0551e-07, -8.4152e-07,  8.0577e-07,  2.2135e-07,  1.4701e-07,\n",
       "                      -4.7796e-07,  5.2044e-07,  2.2653e-09, -7.0618e-07,  1.4517e-07,\n",
       "                       1.1100e-07, -4.7671e-07,  1.6476e-08,  7.9813e-07,  1.0975e-06,\n",
       "                      -2.4273e-07,  1.0612e-07,  1.7355e-07,  1.0534e-07,  1.0785e-07,\n",
       "                      -2.7264e-07,  9.1934e-07, -3.7627e-07, -3.6769e-07,  1.5808e-06,\n",
       "                      -1.0943e-06,  5.2885e-07, -8.0339e-09, -5.7482e-08, -6.6237e-07,\n",
       "                       7.3818e-07, -3.2267e-07, -7.3434e-07, -1.6039e-07,  2.9979e-07,\n",
       "                       1.4976e-07, -1.3301e-07, -4.0172e-07,  3.1610e-07,  9.6887e-07,\n",
       "                      -6.2470e-07,  1.6119e-07,  5.4813e-08,  3.9199e-08,  6.8262e-07,\n",
       "                       4.0209e-07,  2.2083e-07,  9.4777e-08, -1.4090e-07, -3.2555e-07,\n",
       "                       6.8725e-07,  1.8241e-07, -3.0460e-07, -2.0503e-08, -2.2953e-07,\n",
       "                      -3.4481e-07,  2.0229e-08,  9.2232e-08,  2.1947e-07,  1.6924e-07,\n",
       "                      -3.9328e-07,  4.7698e-07, -1.1541e-06,  9.2050e-08,  2.2298e-07,\n",
       "                      -3.4624e-07, -5.2339e-07,  6.2884e-07,  1.7464e-07, -1.0236e-06,\n",
       "                       3.9415e-07, -6.1046e-07,  8.0113e-07,  8.6029e-07,  1.5246e-07,\n",
       "                       5.0858e-07, -3.1620e-07, -1.5958e-06, -6.5552e-07, -4.9580e-08])),\n",
       "             ('batch2.running_mean',\n",
       "              tensor([-0.5063,  0.0658,  0.1237, -0.2722, -0.1693,  0.4204,  0.0354, -0.1981,\n",
       "                       0.0603,  0.0353,  0.4993, -0.1678, -0.1369, -0.0683, -0.3539, -0.3272,\n",
       "                      -0.5471, -0.0442,  0.4870, -0.1201, -0.3743,  0.1545,  0.1691,  0.1661,\n",
       "                      -0.1568,  0.1647, -0.1257, -0.1309,  0.0755,  0.1964,  0.8460, -0.1642,\n",
       "                       0.1243, -0.1022,  0.1377,  0.5173,  0.0584,  0.1772, -0.0786, -0.1174,\n",
       "                      -0.0328,  0.1575,  0.4634, -0.0268, -0.2937,  0.0306,  0.1827, -0.0892,\n",
       "                      -0.3074,  0.0940,  0.1715,  0.0555,  0.2872, -0.1878,  0.0462,  0.2167,\n",
       "                       0.0288, -0.0958,  0.4354, -0.2325, -0.1476, -0.1663, -0.1073, -0.2413,\n",
       "                      -0.0761,  0.1487, -0.1535,  0.1857,  0.0279,  0.0153,  0.0416, -0.3076,\n",
       "                      -0.0780,  0.1264, -0.1747, -0.0647, -0.1510,  0.1537, -0.0833, -0.0169,\n",
       "                       0.3235, -0.0491, -0.1677,  0.1098,  0.1625,  0.0073, -0.0135, -0.0887,\n",
       "                       0.3240,  0.1346, -0.0234, -0.0886, -0.0700, -0.1356, -0.3215,  0.2065,\n",
       "                       0.4089, -0.1794, -0.1197,  0.4325])),\n",
       "             ('batch2.running_var',\n",
       "              tensor([0.1555, 0.0646, 0.0622, 0.1549, 0.0977, 0.1656, 0.1333, 0.3693, 0.0387,\n",
       "                      0.0402, 0.0743, 0.0330, 0.1175, 0.0152, 0.1383, 0.0871, 0.1100, 0.0408,\n",
       "                      0.3082, 0.0931, 0.1293, 0.0654, 0.0573, 0.1574, 0.1291, 0.3217, 0.0479,\n",
       "                      0.3724, 0.0598, 0.0520, 0.2079, 0.0281, 0.0685, 0.0269, 0.0317, 0.2308,\n",
       "                      0.1628, 0.1359, 0.0321, 0.2704, 0.0821, 0.0531, 0.0644, 0.0240, 0.0491,\n",
       "                      0.0933, 0.1392, 0.0441, 0.0734, 0.3282, 0.0340, 0.0569, 0.0976, 0.0308,\n",
       "                      0.1180, 0.0851, 0.1156, 0.0842, 0.0730, 0.0382, 0.2386, 0.1952, 0.0535,\n",
       "                      0.0787, 0.1995, 0.0538, 0.0555, 0.2117, 0.0473, 0.1553, 0.0559, 0.0232,\n",
       "                      0.0364, 0.1463, 0.1040, 0.0279, 0.1569, 0.0468, 0.1239, 0.3711, 0.0791,\n",
       "                      0.0392, 0.0500, 0.0923, 0.1115, 0.0644, 0.0438, 0.0628, 0.0614, 0.0317,\n",
       "                      0.0459, 0.0809, 0.0841, 0.0854, 0.0597, 0.3887, 0.0561, 0.0531, 0.3205,\n",
       "                      0.2006])),\n",
       "             ('batch2.num_batches_tracked', tensor(5000)),\n",
       "             ('linear3.weight',\n",
       "              tensor([[-0.0637, -0.0345, -0.0874,  ..., -0.0941, -0.0619,  0.0440],\n",
       "                      [-0.0242,  0.0799,  0.0206,  ..., -0.0790,  0.0136, -0.0173],\n",
       "                      [ 0.0671, -0.0972, -0.0715,  ...,  0.0137,  0.0875, -0.0843],\n",
       "                      ...,\n",
       "                      [-0.0272,  0.0820, -0.0828,  ...,  0.0803, -0.0081, -0.0968],\n",
       "                      [ 0.0828, -0.0695, -0.0766,  ..., -0.0064, -0.0443, -0.0491],\n",
       "                      [-0.0101,  0.0197,  0.0234,  ...,  0.0445, -0.0183, -0.0813]])),\n",
       "             ('linear3.bias',\n",
       "              tensor([ 0.0329, -0.0381,  0.0676,  0.0020, -0.0063,  0.0267, -0.0700,  0.0649,\n",
       "                       0.0990, -0.0216, -0.0033,  0.0759,  0.0578,  0.0269,  0.0650, -0.0680,\n",
       "                      -0.0332,  0.0114,  0.0552, -0.0195, -0.0611, -0.0011, -0.0297, -0.0611,\n",
       "                      -0.0802,  0.0234,  0.0973, -0.0879,  0.0825,  0.0458,  0.0351,  0.0202,\n",
       "                       0.0146,  0.0589, -0.0995,  0.0316, -0.0007,  0.0948, -0.0640,  0.0351])),\n",
       "             ('batch3.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('batch3.bias',\n",
       "              tensor([-2.4884e-07,  1.5586e-06, -2.4431e-06, -2.4190e-06,  2.0924e-06,\n",
       "                      -1.2151e-06, -7.9744e-07, -7.2943e-07, -1.4286e-06, -2.9294e-06,\n",
       "                      -1.3046e-06, -2.0203e-06, -1.7666e-07, -2.5090e-06, -5.6352e-07,\n",
       "                       1.1448e-06, -1.1247e-07,  5.8356e-07, -3.9519e-06, -1.9218e-06,\n",
       "                      -5.3049e-08,  6.1213e-07,  3.5382e-06, -3.4808e-06, -9.8321e-07,\n",
       "                      -6.8899e-07, -1.4771e-06,  2.5334e-06, -3.0452e-06, -5.1514e-07,\n",
       "                      -2.5187e-06,  1.3302e-06,  3.1339e-06, -1.6064e-07,  7.5661e-07,\n",
       "                      -1.1919e-08, -2.5979e-07,  2.4924e-06,  3.0473e-06, -8.1018e-07])),\n",
       "             ('batch3.running_mean',\n",
       "              tensor([-9.3500e-04, -5.9902e-01,  4.1715e-01,  1.4749e-01,  2.3094e-01,\n",
       "                       1.8619e-01, -1.5214e-01,  1.2994e-01,  1.9057e-01, -2.4725e-01,\n",
       "                      -3.3799e-01, -8.6757e-02,  2.1966e-01, -1.1548e-01,  4.3301e-01,\n",
       "                      -2.5757e-01,  2.4456e-01, -8.2611e-02, -1.2240e-01,  2.9056e-01,\n",
       "                       1.3048e-01,  1.5789e-02, -2.0351e-01, -1.4625e-01, -1.0354e-01,\n",
       "                      -2.2461e-01,  9.9927e-02, -4.0602e-01,  3.1608e-02,  1.2915e-01,\n",
       "                       2.3557e-04,  1.3078e-01, -1.0112e-01, -2.8686e-01, -3.8577e-01,\n",
       "                      -1.1757e-01,  2.2189e-01, -3.1789e-01,  2.6991e-01, -4.5076e-02])),\n",
       "             ('batch3.running_var',\n",
       "              tensor([0.1214, 0.1216, 0.1593, 0.1219, 0.1146, 0.0779, 0.0755, 0.1295, 0.1719,\n",
       "                      0.3926, 0.0964, 0.0908, 0.0785, 0.0958, 0.1433, 0.0580, 0.0805, 0.0412,\n",
       "                      0.0595, 0.1150, 0.0539, 0.1278, 0.0581, 0.0843, 0.0899, 0.1049, 0.0875,\n",
       "                      0.0700, 0.0385, 0.0661, 0.1911, 0.1702, 0.2593, 0.0891, 0.0638, 0.0634,\n",
       "                      0.4080, 0.1203, 0.3464, 0.0887])),\n",
       "             ('batch3.num_batches_tracked', tensor(5000)),\n",
       "             ('linear4.weight',\n",
       "              tensor([[-0.0120,  0.0592, -0.1182,  0.1282,  0.1475, -0.1007,  0.1400, -0.0460,\n",
       "                        0.1054, -0.1499, -0.0888, -0.1189, -0.0085, -0.1081,  0.0509,  0.0424,\n",
       "                       -0.0540,  0.0227, -0.1324,  0.1330, -0.0533, -0.1015,  0.1270, -0.1076,\n",
       "                        0.0923, -0.0847, -0.0671,  0.0955, -0.1404, -0.0803, -0.1079,  0.1525,\n",
       "                        0.1243, -0.0075,  0.0585,  0.0189, -0.0153,  0.1343,  0.1300, -0.0477]])),\n",
       "             ('linear4.bias', tensor([-0.1269]))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.2564111351966858\n",
      "epoch 200, loss 0.25637346506118774\n",
      "epoch 400, loss 0.2563360929489136\n"
     ]
    }
   ],
   "source": [
    "#we can call train to update our loaded model\n",
    "training_results = train(model1, criterion, x_train, x_val, y_train, y_val, optimizer, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
