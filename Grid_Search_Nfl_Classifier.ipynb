{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as ply\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_day</th>\n",
       "      <th>date_month</th>\n",
       "      <th>date_year</th>\n",
       "      <th>Week</th>\n",
       "      <th>Spread</th>\n",
       "      <th>OverUnder</th>\n",
       "      <th>days_rest</th>\n",
       "      <th>team_season_wins</th>\n",
       "      <th>team_season_losses</th>\n",
       "      <th>prev_week_total</th>\n",
       "      <th>...</th>\n",
       "      <th>Sun</th>\n",
       "      <th>Thu</th>\n",
       "      <th>Tue</th>\n",
       "      <th>Wed</th>\n",
       "      <th>spread_Lost</th>\n",
       "      <th>spread_Push</th>\n",
       "      <th>spread_Won</th>\n",
       "      <th>OU_Over</th>\n",
       "      <th>OU_Push</th>\n",
       "      <th>OU_Under</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>45.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>45.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>45.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117</th>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>2.5</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3118</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>6.5</td>\n",
       "      <td>43.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119</th>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>43.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3120</th>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>14</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3121</th>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2868 rows × 539 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_day  date_month  date_year  Week  Spread  OverUnder  days_rest  \\\n",
       "1           20           9       2015     2    -2.0       46.0        7.0   \n",
       "2           27           9       2015     3    -6.5       45.0        7.0   \n",
       "3           11          10       2015     4    -3.0       45.5       14.0   \n",
       "4           18          10       2015     5    -3.5       45.0        7.0   \n",
       "5           26          10       2015     6    -8.0       49.0        8.0   \n",
       "...        ...         ...        ...   ...     ...        ...        ...   \n",
       "3117        26          11       2020    11     2.5       46.0        4.0   \n",
       "3118         7          12       2020    12     6.5       43.5       11.0   \n",
       "3119        13          12       2020    13     3.0       43.5        6.0   \n",
       "3120        20          12       2020    14     6.0       44.5        7.0   \n",
       "3121        27          12       2020    15     1.0       41.5        7.0   \n",
       "\n",
       "      team_season_wins  team_season_losses  prev_week_total  ...  Sun  Thu  \\\n",
       "1                  1.0                 0.0             50.0  ...    1    0   \n",
       "2                  2.0                 0.0             71.0  ...    1    0   \n",
       "3                  3.0                 0.0             54.0  ...    1    0   \n",
       "4                  4.0                 0.0             59.0  ...    1    0   \n",
       "5                  4.0                 1.0             38.0  ...    0    0   \n",
       "...                ...                 ...              ...  ...  ...  ...   \n",
       "3117               3.0                 7.0             29.0  ...    0    1   \n",
       "3118               4.0                 7.0             57.0  ...    0    0   \n",
       "3119               5.0                 7.0             40.0  ...    1    0   \n",
       "3120               6.0                 7.0             38.0  ...    1    0   \n",
       "3121               6.0                 8.0             35.0  ...    1    0   \n",
       "\n",
       "      Tue  Wed  spread_Lost  spread_Push  spread_Won  OU_Over  OU_Push  \\\n",
       "1       0    0            0            0           1        1        0   \n",
       "2       0    0            0            0           1        1        0   \n",
       "3       0    0            0            0           1        1        0   \n",
       "4       0    0            1            0           0        0        0   \n",
       "5       0    0            0            1           0        0        0   \n",
       "...   ...  ...          ...          ...         ...      ...      ...   \n",
       "3117    0    0            0            0           1        1        0   \n",
       "3118    0    0            0            0           1        0        0   \n",
       "3119    0    0            0            0           1        0        0   \n",
       "3120    0    0            0            0           1        0        0   \n",
       "3121    0    0            1            0           0        0        0   \n",
       "\n",
       "      OU_Under  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            1  \n",
       "5            1  \n",
       "...        ...  \n",
       "3117         0  \n",
       "3118         1  \n",
       "3119         1  \n",
       "3120         1  \n",
       "3121         1  \n",
       "\n",
       "[2868 rows x 539 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\cpilo\\Downloads\\df_all_teams.csv\")\n",
    "\n",
    "df_team_data = pd.get_dummies(df.team_abbr)\n",
    "df_opponent_data = pd.get_dummies(df.opponent_abbr)\n",
    "col_names = []\n",
    "for i in df_opponent_data.columns:\n",
    "    col_names.append('opp_'+i)\n",
    "df_opponent_data.columns = col_names\n",
    "game_city = pd.get_dummies(df.game_city)\n",
    "game_time = pd.get_dummies(df.Time)\n",
    "game_day = pd.get_dummies(df.Day)\n",
    "vs_line = pd.get_dummies(df['vs. Line'])\n",
    "qb_1 = pd.get_dummies(df['QB1'])\n",
    "opp_qb_1 = pd.get_dummies(df['opp_QB1'])\n",
    "skill_1 = pd.get_dummies(df['Skill1'])\n",
    "opp_skill_1 = pd.get_dummies(df['opp_skill1'])\n",
    "skill_2 = pd.get_dummies(df['Skill2'])\n",
    "opp_skill_2 = pd.get_dummies(df['opp_skill2'])\n",
    "skill_3 = pd.get_dummies(df['Skill3'])\n",
    "opp_skill_3 = pd.get_dummies(df['opp_skill3'])\n",
    "skill_4 = pd.get_dummies(df['Skill4'])\n",
    "opp_skill_4 = pd.get_dummies(df['opp_skill4'])\n",
    "skill_5 = pd.get_dummies(df['Skill5'])\n",
    "opp_skill_5 = pd.get_dummies(df['opp_skill5'])\n",
    "oline_1 = pd.get_dummies(df['Oline1'])\n",
    "opp_oline_1 = pd.get_dummies(df['opp_oline1'])\n",
    "oline_2 = pd.get_dummies(df['Oline2'])\n",
    "opp_oline_2 = pd.get_dummies(df['opp_oline2'])\n",
    "oline_3 = pd.get_dummies(df['Oline3'])\n",
    "opp_oline_3 = pd.get_dummies(df['opp_oline3'])\n",
    "oline_4 = pd.get_dummies(df['Oline4'])\n",
    "opp_oline_4 = pd.get_dummies(df['opp_oline4'])\n",
    "oline_5 = pd.get_dummies(df['Oline5'])\n",
    "opp_oline_5 = pd.get_dummies(df['opp_oline5'])\n",
    "dline_1 = pd.get_dummies(df['Dline1'])\n",
    "opp_dline_1 = pd.get_dummies(df['opp_dline1'])\n",
    "dline_2 = pd.get_dummies(df['Dline2'])\n",
    "opp_dline_2 = pd.get_dummies(df['opp_dline2'])\n",
    "dline_3 = pd.get_dummies(df['Dline3'])\n",
    "opp_dline_3 = pd.get_dummies(df['opp_dline3'])\n",
    "lb_1 = pd.get_dummies(df['LB1'])\n",
    "opp_lb_1 = pd.get_dummies(df['opp_lb1'])\n",
    "lb_2 = pd.get_dummies(df['LB2'])\n",
    "opp_lb_2 = pd.get_dummies(df['opp_lb2'])\n",
    "lb_3 = pd.get_dummies(df['LB3'])\n",
    "opp_lb_3 = pd.get_dummies(df['opp_lb3'])\n",
    "lb_4 = pd.get_dummies(df['LB4'])\n",
    "opp_lb_4 = pd.get_dummies(df['opp_lb4'])\n",
    "cb_1 = pd.get_dummies(df['CB1'])\n",
    "opp_cb_1 = pd.get_dummies(df['opp_cb1'])\n",
    "cb_2 = pd.get_dummies(df['CB2'])\n",
    "opp_cb_2 = pd.get_dummies(df['opp_cb2'])\n",
    "ss = pd.get_dummies(df['SS'])\n",
    "opp_ss = pd.get_dummies(df['opp_ss'])\n",
    "fs = pd.get_dummies(df['FS'])\n",
    "opp_fs = pd.get_dummies(df['opp_fs'])\n",
    "\n",
    "col_names = []\n",
    "for i in vs_line.columns:\n",
    "    col_names.append('spread_'+i)\n",
    "vs_line.columns = col_names\n",
    "OU_result = pd.get_dummies(df['OU Result'])\n",
    "col_names = []\n",
    "for i in OU_result.columns:\n",
    "    col_names.append('OU_'+i)\n",
    "OU_result.columns = col_names\n",
    "model_vars = ['date_day', 'date_month', 'date_year', 'Week', 'Spread', 'OverUnder','days_rest', 'team_season_wins', \n",
    "              'team_season_losses', 'prev_week_total', 'prev_week_ptsdiff',\n",
    "              'prev_week_ydsoff', 'prev_week_ydsdef', 'prev_week_ptsoff', \n",
    "              'prev_week_ptsdef', 'prev_week_TOoff', 'prev_week_TOdef', \n",
    "              'prev_week_seasptsdiff', 'prev_week_seasptsoff', \n",
    "              'prev_week_seasptsdef', 'prev_week_seasydsoff', 'prev_week_seasydsdef', \n",
    "              'prev_week_seasTOoff', 'prev_week_seasTOdef', 'prev_week_avgpts', 'prev_week_TOdiff',\n",
    "              'prev_week_avgTOdiff', 'prev_week_avgptsdiff', 'prev_week_avgptsoff', \n",
    "              'prev_week_avgptsdef', 'prev_week_avgydsoff', 'prev_week_avgydsdef', \n",
    "              'prev_week_avgTOoff', 'prev_week_avgTOdef', 'Opp_days_rest', 'Opp_team_season_wins',\n",
    "              'Opp_team_season_losses', 'opp_prev_week_total', 'opp_prev_week_pts_diff', \n",
    "              'opp_prev_week_ydsoff', 'opp_prev_week_ydsdef', 'opp_prev_week_ptsoff', 'opp_prev_week_ptsdef', \n",
    "              'opp_prev_week_TOoff', 'opp_prev_week_TOdef', 'opp_prev_week_seasptsdiff', \n",
    "              'opp_prev_week_seasptsoff', 'opp_prev_week_seasptsdef', 'opp_prev_week_seasydsoff',\n",
    "              'opp_prev_week_seasydsdef', 'opp_prev_week_seasTOoff', 'opp_prev_week_seasTOdef',\n",
    "              'opp_prev_week_avgpts', 'opp_prev_week_TOdiff', 'opp_prev_week_avgTOdiff',\n",
    "              'opp_prev_week_avgptsdiff', 'opp_prev_week_avgptsoff', 'opp_prev_week_avgptsdef',\n",
    "              'opp_prev_week_avgydsoff', 'opp_prev_week_avgydsdef', 'opp_prev_week_avgTOoff',\n",
    "              'opp_prev_week_avgTOdef', 'prev_week_avgspreaddif', 'prev_week_avgoverdif', 'prev_week_avgspread',\n",
    "              'prev_week_avgover', 'prev_week_seasspreadwin', 'prev_week_seasspreadloss', 'prev_week_seasspreadtie',\n",
    "              'prev_week_seasoverwin', 'prev_week_seasoverloss', 'prev_week_seasovertie', 'prev_week_seastotspread',\n",
    "              'prev_week_seastotover', 'prev_week_spreadwin', 'prev_week_spreadloss', 'prev_week_spreadtie',\n",
    "              'prev_week_overwin', 'prev_week_overloss', 'prev_week_overtie', 'prev_week_spreaddiff', \n",
    "              'prev_week_overdiff', 'opp_prev_week_avgspreaddif', 'opp_prev_week_avgoverdif',\n",
    "              'prev_week_streakw', 'prev_week_streakl', 'prev_week_streakspreadw', 'prev_week_streakspreadl',\n",
    "              'prev_week_streakoverw', 'prev_week_streakoverl', 'prev_week_avgspread', 'prev_week_avgover', \n",
    "              'prev_week_seasspreadwin', 'prev_week_seasspreadloss', 'prev_week_seasspreadtie', 'prev_week_seasoverwin',\n",
    "              'prev_week_seasoverloss', 'prev_week_seasovertie', 'prev_week_seastotspread', 'prev_week_seastotover',\n",
    "              'prev_week_spreadwin', 'prev_week_spreadloss', 'prev_week_spreadtie', 'prev_week_overwin',\n",
    "              'prev_week_overloss', 'prev_week_overtie', 'prev_week_spreaddiff', 'prev_week_overdiff',              \n",
    "              'opp_prev_week_avgspread', 'opp_prev_week_avgover', 'opp_prev_week_seasspreadwin', \n",
    "              'opp_prev_week_seasspreadloss', 'opp_prev_week_seasspreadtie', 'opp_prev_week_seasoverwin',\n",
    "              'opp_prev_week_seasoverloss', 'opp_prev_week_seasovertie', 'opp_prev_week_seastotspread', \n",
    "              'opp_prev_week_seastotover', 'opp_prev_week_spreadwin', 'opp_prev_week_spreadloss', \n",
    "              'opp_prev_week_spreadtie', 'opp_prev_week_overwin', 'opp_prev_week_overloss', \n",
    "              'opp_prev_week_overtie', 'opp_prev_week_spreaddiff', 'opp_prev_week_overdiff',\n",
    "              'opp_prev_week_avgspread', 'opp_prev_week_avgover', 'opp_prev_week_seasspreadwin',\n",
    "              'opp_prev_week_seasspreadloss', 'opp_prev_week_seasspreadtie', 'opp_prev_week_seasoverwin',\n",
    "              'opp_prev_week_seasoverloss', 'opp_prev_week_seasovertie', 'opp_prev_week_seastotspread', \n",
    "              'opp_prev_week_seastotover', 'opp_prev_week_spreadwin', 'opp_prev_week_spreadloss',\n",
    "              'opp_prev_week_spreadtie', 'opp_prev_week_overwin', 'opp_prev_week_overloss',\n",
    "              'opp_prev_week_overtie', 'opp_prev_week_spreaddiff', 'opp_prev_week_overdiff',\n",
    "              'opp_prev_week_streakw', 'opp_prev_week_streakl', 'opp_prev_week_streakspreadw',\n",
    "              'opp_prev_week_streakspreadl', 'opp_prev_week_streakoverw', 'opp_prev_week_streakoverl']\n",
    "              \n",
    "\n",
    "df_rfc_data = df[model_vars]\n",
    "df_final_data = pd.concat([df_rfc_data, df_team_data, df_opponent_data, game_city, qb_1, opp_qb_1,\n",
    "                           skill_1, opp_skill_1, skill_2, opp_skill_2, skill_3, opp_skill_3,\n",
    "                           skill_4, opp_skill_4, skill_5, opp_skill_5, oline_1, opp_oline_1,\n",
    "                           oline_2, opp_oline_2, oline_3, opp_oline_3, oline_4, opp_oline_4,\n",
    "                           oline_5, opp_oline_5, dline_1, opp_dline_1, dline_2, opp_dline_2,\n",
    "                           dline_3, opp_dline_3, lb_1, opp_lb_1, lb_2, opp_lb_2, lb_3, opp_lb_3, \n",
    "                           lb_4, opp_lb_4, cb_1, opp_cb_1, cb_2, opp_cb_2, ss, opp_ss, fs, opp_fs,\n",
    "                           game_time, game_day, vs_line, OU_result], axis = 'columns')\n",
    "\n",
    "df_final_data = df_final_data[df_final_data['Week'] != 1]\n",
    "df_final_data = df_final_data.dropna(axis = 'index')\n",
    "df_train = df_final_data[df_final_data['date_year'] < 2021]\n",
    "df_test = df_final_data[df_final_data['date_year'] >= 2021]\n",
    "df_train = df_train.replace({True:1, False:0})\n",
    "df_test = df_test.replace({True:1, False:0})\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_y = df_train['spread_Won']\n",
    "df_train_x = df_train.drop(['spread_Won', 'spread_Lost', 'spread_Push', 'OU_Over', 'OU_Push', 'OU_Under'], axis = 'columns')\n",
    "df_test_y = df_test['spread_Won']\n",
    "df_test_x = df_test.drop(['spread_Won', 'spread_Lost', 'spread_Push', 'OU_Over', 'OU_Push', 'OU_Under'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_train_x = scaler.fit_transform(df_train_x)\n",
    "df_train_x = pd.DataFrame(df_train_x, columns = df_test_x.columns)\n",
    "df_test_x = scaler.fit_transform(df_test_x)\n",
    "df_test_x = pd.DataFrame(df_test_x, columns = df_train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "600 fits failed out of a total of 1500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\cpilo\\Hello\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.50766915 0.50732376        nan        nan        nan 0.51255268\n",
      " 0.50905863 0.50905863 0.51811968        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.51255268\n",
      " 0.5128932  0.5128932  0.52544223        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.51255268\n",
      " 0.52369764 0.52369764 0.52195244        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.51255268\n",
      " 0.52509197 0.52509197 0.5233492         nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.51255268\n",
      " 0.52056418 0.52056418 0.51812333        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.51150738\n",
      " 0.51324528 0.51324528 0.51394275        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.50940706\n",
      " 0.50975792 0.50940949 0.51219573        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.51743194\n",
      " 0.50627299 0.50696986 0.50906045        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.51255207\n",
      " 0.51115347 0.51185095 0.51150191        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.50871445\n",
      " 0.50906227 0.50871506 0.50766733        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.51010757\n",
      " 0.50871506 0.50801941 0.50766976        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.51045722\n",
      " 0.50871688 0.50767159 0.50871688        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.5090641\n",
      " 0.50732194 0.50801941 0.50732194        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.50732194\n",
      " 0.5069729  0.50802002 0.50662386        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.50662386\n",
      " 0.50627543 0.50836784 0.50627543        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.50627543\n",
      " 0.50627603 0.50941618 0.50662386        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.50697229\n",
      " 0.50732133 0.50906653 0.50662386        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.50557917\n",
      " 0.50662386 0.50837027 0.50662386        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.50732072\n",
      " 0.50662386 0.50941496 0.50662386        nan        nan        nan\n",
      " 0.50766915 0.50732376        nan        nan        nan 0.50766915\n",
      " 0.50697229 0.50836784 0.50662386        nan        nan        nan\n",
      " 0.50906471 0.49303622 0.50835629 0.50417814 0.48814419 0.50243477\n",
      " 0.51150677 0.50625901 0.50487987 0.51777247 0.50942469 0.50801394\n",
      " 0.50348675 0.51952557 0.50104165 0.50975853 0.51359007 0.51255389\n",
      " 0.50627543 0.50035208 0.51638786 0.49617394 0.50173365 0.50138765\n",
      " 0.50522952 0.49964914 0.49895288 0.50766915 0.50034235 0.50557795\n",
      " 0.50522283 0.49233206 0.49650473 0.51289016 0.50486589 0.5055743\n",
      " 0.50172939 0.51046147 0.51324467 0.50697107 0.51917957 0.49407179\n",
      " 0.5062584  0.49650717 0.50976522 0.49476926 0.50243234 0.50348371\n",
      " 0.50001034 0.52788429 0.50278928 0.51047911 0.51848088 0.51778402\n",
      " 0.50592152 0.50418118 0.51952071 0.51150434 0.50905619 0.49440928]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "\n",
    "# Create param grid.\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     #we will try 4 different regularization penalties\n",
    "     'classifier__penalty' : ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'classifier__C' : np.logspace(-4, 4, 20),\n",
    "     #We will try 3 different solvers\n",
    "    'classifier__solver' : ['newton-cg', 'lbfgs', 'liblinear']},\n",
    "    {'classifier' : [RandomForestClassifier()],\n",
    "    'classifier__n_estimators' : list(range(10,101,10)),\n",
    "    'classifier__max_features' : list(range(6,32,5))}\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "\n",
    "# Fit on data\n",
    "\n",
    "best_clf = clf.fit(df_train_x, df_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': RandomForestClassifier(max_features=26),\n",
       " 'classifier__max_features': 26,\n",
       " 'classifier__n_estimators': 100}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5269360269360269"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.score(df_test_x, df_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_classifier__penalty</th>\n",
       "      <th>param_classifier__solver</th>\n",
       "      <th>param_classifier__max_features</th>\n",
       "      <th>param_classifier__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>3.386279</td>\n",
       "      <td>0.063042</td>\n",
       "      <td>0.050750</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>RandomForestClassifier(max_features=26)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>100</td>\n",
       "      <td>{'classifier': RandomForestClassifier(max_feat...</td>\n",
       "      <td>0.541812</td>\n",
       "      <td>0.554007</td>\n",
       "      <td>0.515679</td>\n",
       "      <td>0.523560</td>\n",
       "      <td>0.504363</td>\n",
       "      <td>0.527884</td>\n",
       "      <td>0.017873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.724400</td>\n",
       "      <td>0.032831</td>\n",
       "      <td>0.018039</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>l2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>0.559233</td>\n",
       "      <td>0.547038</td>\n",
       "      <td>0.501742</td>\n",
       "      <td>0.518325</td>\n",
       "      <td>0.500873</td>\n",
       "      <td>0.525442</td>\n",
       "      <td>0.023767</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.816819</td>\n",
       "      <td>0.204609</td>\n",
       "      <td>0.021874</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>0.559233</td>\n",
       "      <td>0.543554</td>\n",
       "      <td>0.508711</td>\n",
       "      <td>0.518325</td>\n",
       "      <td>0.495637</td>\n",
       "      <td>0.525092</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.462525</td>\n",
       "      <td>0.038485</td>\n",
       "      <td>0.021499</td>\n",
       "      <td>0.005449</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>0.559233</td>\n",
       "      <td>0.543554</td>\n",
       "      <td>0.508711</td>\n",
       "      <td>0.518325</td>\n",
       "      <td>0.495637</td>\n",
       "      <td>0.525092</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.441522</td>\n",
       "      <td>0.103938</td>\n",
       "      <td>0.023901</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>0.567944</td>\n",
       "      <td>0.538328</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516579</td>\n",
       "      <td>0.495637</td>\n",
       "      <td>0.523698</td>\n",
       "      <td>0.026726</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.041387</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>11.288379</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.032573</td>\n",
       "      <td>0.009460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>11.288379</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.042153</td>\n",
       "      <td>0.004284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>1438.449888</td>\n",
       "      <td>l1</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.046475</td>\n",
       "      <td>0.009946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>11.288379</td>\n",
       "      <td>l1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.038075</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>545.559478</td>\n",
       "      <td>l1</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "289       3.386279      0.063042         0.050750        0.002141   \n",
       "20        0.724400      0.032831         0.018039        0.004808   \n",
       "42        1.816819      0.204609         0.021874        0.003894   \n",
       "43        0.462525      0.038485         0.021499        0.005449   \n",
       "30        1.441522      0.103938         0.023901        0.003710   \n",
       "..             ...           ...              ...             ...   \n",
       "153       0.041387      0.012660         0.000000        0.000000   \n",
       "154       0.032573      0.009460         0.000000        0.000000   \n",
       "208       0.042153      0.004284         0.000000        0.000000   \n",
       "147       0.046475      0.009946         0.000000        0.000000   \n",
       "195       0.038075      0.003942         0.000000        0.000000   \n",
       "\n",
       "                            param_classifier param_classifier__C  \\\n",
       "289  RandomForestClassifier(max_features=26)                 NaN   \n",
       "20                      LogisticRegression()            0.000264   \n",
       "42                      LogisticRegression()            0.001833   \n",
       "43                      LogisticRegression()            0.001833   \n",
       "30                      LogisticRegression()            0.000695   \n",
       "..                                       ...                 ...   \n",
       "153                     LogisticRegression()           11.288379   \n",
       "154                     LogisticRegression()           11.288379   \n",
       "208                     LogisticRegression()         1438.449888   \n",
       "147                     LogisticRegression()           11.288379   \n",
       "195                     LogisticRegression()          545.559478   \n",
       "\n",
       "    param_classifier__penalty param_classifier__solver  \\\n",
       "289                       NaN                      NaN   \n",
       "20                         l2                liblinear   \n",
       "42                         l2                newton-cg   \n",
       "43                         l2                    lbfgs   \n",
       "30                         l2                newton-cg   \n",
       "..                        ...                      ...   \n",
       "153                elasticnet                newton-cg   \n",
       "154                elasticnet                    lbfgs   \n",
       "208                        l1                    lbfgs   \n",
       "147                        l1                newton-cg   \n",
       "195                        l1                newton-cg   \n",
       "\n",
       "    param_classifier__max_features param_classifier__n_estimators  \\\n",
       "289                             26                            100   \n",
       "20                             NaN                            NaN   \n",
       "42                             NaN                            NaN   \n",
       "43                             NaN                            NaN   \n",
       "30                             NaN                            NaN   \n",
       "..                             ...                            ...   \n",
       "153                            NaN                            NaN   \n",
       "154                            NaN                            NaN   \n",
       "208                            NaN                            NaN   \n",
       "147                            NaN                            NaN   \n",
       "195                            NaN                            NaN   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "289  {'classifier': RandomForestClassifier(max_feat...           0.541812   \n",
       "20   {'classifier': LogisticRegression(), 'classifi...           0.559233   \n",
       "42   {'classifier': LogisticRegression(), 'classifi...           0.559233   \n",
       "43   {'classifier': LogisticRegression(), 'classifi...           0.559233   \n",
       "30   {'classifier': LogisticRegression(), 'classifi...           0.567944   \n",
       "..                                                 ...                ...   \n",
       "153  {'classifier': LogisticRegression(), 'classifi...                NaN   \n",
       "154  {'classifier': LogisticRegression(), 'classifi...                NaN   \n",
       "208  {'classifier': LogisticRegression(), 'classifi...                NaN   \n",
       "147  {'classifier': LogisticRegression(), 'classifi...                NaN   \n",
       "195  {'classifier': LogisticRegression(), 'classifi...                NaN   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  \\\n",
       "289           0.554007           0.515679           0.523560   \n",
       "20            0.547038           0.501742           0.518325   \n",
       "42            0.543554           0.508711           0.518325   \n",
       "43            0.543554           0.508711           0.518325   \n",
       "30            0.538328           0.500000           0.516579   \n",
       "..                 ...                ...                ...   \n",
       "153                NaN                NaN                NaN   \n",
       "154                NaN                NaN                NaN   \n",
       "208                NaN                NaN                NaN   \n",
       "147                NaN                NaN                NaN   \n",
       "195                NaN                NaN                NaN   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "289           0.504363         0.527884        0.017873                1  \n",
       "20            0.500873         0.525442        0.023767                2  \n",
       "42            0.495637         0.525092        0.023187                3  \n",
       "43            0.495637         0.525092        0.023187                3  \n",
       "30            0.495637         0.523698        0.026726                5  \n",
       "..                 ...              ...             ...              ...  \n",
       "153                NaN              NaN             NaN              181  \n",
       "154                NaN              NaN             NaN              181  \n",
       "208                NaN              NaN             NaN              181  \n",
       "147                NaN              NaN             NaN              181  \n",
       "195                NaN              NaN             NaN              181  \n",
       "\n",
       "[300 rows x 19 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The best model is a random forest classifier with 26 features\n",
    "#The best performing logistic regression model was a liblinear model with l2 regularization\n",
    "pd.DataFrame(best_clf.cv_results_).sort_values(by=['rank_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will now evaluate the performance of our best model\n",
    "yhat = best_clf.best_estimator_.predict(df_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Confusion Matrix ===\n",
      "[[183 117]\n",
      " [164 130]]\n",
      "\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.61      0.57       300\n",
      "           1       0.53      0.44      0.48       294\n",
      "\n",
      "    accuracy                           0.53       594\n",
      "   macro avg       0.53      0.53      0.52       594\n",
      "weighted avg       0.53      0.53      0.52       594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(df_test_y, yhat))\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(df_test_y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
